# (PART) Misc{-}

# Classification with knn and decision trees
**Aims**

- to introduce classification with knn and decision trees

**Learning outcomes**

- to understand the concepts of splitting data into training, validation and test set
- to be able to calculate overall and class specific classification rates
- to use knn() function to select run the optimal value of k and build knn classifier 
- to use rpart() function to fit and optimize a decision tree
- to use knn and a decision tree for prediction

## Classification
- Classification methods are prediction models and algorithms use to classify or categorize objects based on their measurements
- They belong under **supervised learning** as we usually start off with **labeled** data, i.e. observations with measurements for which we know the label (class) of
- If we have a pair $\{\mathbf{x_i}, g_i\}$ for each observation $i$, with $g_i \in \{1, \dots, G\}$ being the class label, where $G$ is the number of different classes and $\mathbf{x_i}$ a set of exploratory variables, that can be continuous, categorical or a mix of both, then we want to find a **classification rule** $f(.)$ (model) such that $$f(\mathbf{x_i})=g_i$$

## Evaluating Classification Model Performance
- Once we have a classification model we need some way of evaluating how well it works and how it compares to other models
- There are few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model
- Common measures include: correct (overall) classification rate, Missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves

**Correct (miss)classification rate**

- the simplest way to evaluate in which we count for all the $n$ predictions how many times we got the classification right 
$$Correct\; Classifcation \; Rate = \frac{\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}$$ where
$1[]$ is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise
- Missclassification Rate = 1 - Correct Classification Rate

**Class specific rates and cross classification table**
$$CCR \; for \; class\; j =  \frac{number \; of \; observations \; in \; class \; j \; that \; were \; correctly \; classified}{number \; of \; observations \; in \; class \; j} = \\ \sum_{i:g_i=j}{\frac{1[f(\mathbf{x_i})\neq j]}{n_j} = \frac{n_j-k_j}{n_j}}$$

**Example**
```{r, collapse=TRUE}
# Example data
true.clas <- c(1, 1, 1, 1, 1, 1, 2, 2, 2, 2)
pred.class <-  c(1, 1, 2, 1, 1, 2, 1, 1, 2, 2)

# correct classification rate
n <- length(true.clas)
ccr <- sum(true.clas == pred.class)/n
print(ccr)

# cross classification table
tab.pred <- table(true.clas, pred.class)
print(tab.pred)

# cross classification rate
# we divide each row by its sum (using sweep function)
tab.rate <- sweep(tab.pred, 1, apply(tab.pred, 1, sum), "/")
tab.rate <- round(tab.rate, 2)
print(tab.rate)

```


## Data splitting
- part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible
- but the results may not be generalizable to future data due to the added complexity modeling noise that is unique to a particular dataset (overfitting)
- to deal with overconfident estimation of future performance we randomly split data into training data, validation data and test data
- common split strategy are 50%/25%/25% and 33%/33%/33% for training/validation/test
- **training data**: this is data to give fit (train) the classification model, i.e. derive the classification rule
- **validation data**: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters
- **test data**: this data is used to give an estimate of future prediction performance for the model and parameters chosen

## Cross validation
- the could happen that despite random splitting in train/validation/test dataset one of the subsets does not represent data (i.e. gets all the difficult observation to classify)
- or that we do not have enough data in each subset after performing the split
- In **K-fold cross-validation** we split data into $K$ roughly equal-sized parts
- We start by setting the validation data to be the first set of data and the training data to be all other sets
- We estimate the validation error rate / correct classification rate for the split
- We then repeat the process $K-1$ times, each time with a different part of the data set to be the validation data and the remainder being the training data
- We finish with $K$ different error of correct classification rates
- In this way, every data point has its class membership predicted once
- The final reporter error rate is usually the average of $K$ error rates

Now we know how to assess our classification models. Let's try it out on two methods, k-nearest neighbors and decision tree

## k-nearest neighbours
- k-nearest neighbours (knn) is a non-parametric classification method, i.e. we do not have to assume a parametric model for the data of the classes
- there is no need to worry about the diagnostic tests for

**Algorithm**

- Decide on the value of $k$
- Calculate the distance between the query-instance (new observation) and all the training samples
- Sort the distances and determine the nearest neighbours based on the k-th minimum distance
- Gather the categories of the nearest neighbours
- Use simple majority of the categories of nearest neighbours as the prediction value of the new observation

_Euclidean distance is a classic distance used with knn; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhatan distance, maximum distance etc._


```{r, fig.align="center", echo=F, fig.width=5, fig.height=5, fig.cap="An example of k-nearest neighbours algorithm with k=3; in the top new observation (blue) is closest to three red triangales and thus classified as a red triangle; in the bottom, a new observation (blue) is closest to 2 black dots and 1 red triangle thus classified as a black dot (majority vote)"}
# Example data
n1 <- 10
n2 <- 10
set.seed(1)
x <- c(rnorm(n1, mean=0, sd=1), rnorm(n2, mean=0.5, sd=1))
y <- rnorm(n1+n2, mean=0, sd=1)

group <- rep(1, (n1+n2))
group[1:n1] <- 0
idx.1 <- which(group==0)
idx.2 <- which(group==1)

# new points
p1 <- c(1.5, 0.5)
p2 <- c(0, 0.6)

# distance 
dist.1 <- c()
dist.2 <- c()
for (i in 1:length(x))
{
  dist.1[i] <- round(sqrt((p1[1]-x[i])^2 + (p1[2]-y[i])^2),2)
  dist.2[i] <- round(sqrt((p2[1]-x[i])^2 + (p2[2]-y[i])^2),2)
}

# find nearest friends
n.idx1 <- order(dist.1)
n.idx2 <- order(dist.2)


par(mfrow=c(2,2), mar=c(2, 2, 2, 2) + 0.1)
# a) 
plot(x[idx.1],y[idx.1], pch=1, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
points(p1[1], p1[2], pch=13, col="blue", cex=2)
# b) 
plot(x[idx.1],y[idx.1], pch=1, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
points(p1[1], p1[2], pch=13, col="blue", cex=2)
points(x[n.idx1[1:3]], y[n.idx1[1:3]], pch=17, col="red")
# c) 
plot(x[idx.1],y[idx.1], pch=1, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
points(p2[1], p2[2], pch=13, col="blue", cex=2)
# d)
plot(x[idx.1],y[idx.1], pch=1, las=1, xlim=c(min(x), max(x)), ylim=c(min(y), max(y)), xlab="x", ylab="y")
points(x[idx.2], y[idx.2], pch=2, col="red")
points(p2[1], p2[2], pch=13, col="blue", cex=2)
points(x[n.idx2[1]], y[n.idx2[1]], pch=17, col="red")
points(x[n.idx2[2:3]], y[n.idx2[2:3]], pch=19, col="black")

```

**choosing k**

- for problems with 2 classes, choose an odd number of k to avoid ties
- use validation data to fit the model for a series of $k$ values
- pick the value of $k$ which results in the best model (as assessed by the method of choice, e.g. overall classification rate)

Let's see how it works in practice on a classical iris dataset containing measurements on petals and sepals as well as species information (setosa, versicolor, virginica)
```{r, collapse=TRUE, fig.align="center", fig.width=6, fig.height=4}
# library with knn() function
library(class)

# preview iris dataset
head(iris)

# summary statistics
summary(iris)

# split data into train 50%, validation 25% and test dataset 25%
set.seed(5)
n <- nrow(iris) # no. of observations
idx.train <- sample(c(1:n), round(n/2))
idx.valid <- sample(c(1:n)[-idx.train], round(n/4))
idx.test <- setdiff(c(1:n), c(idx.train, idx.valid))

data.train <- iris[idx.train,]
data.valid <- iris[idx.valid,]
data.test <- iris[idx.test,]

dim(data.train)
dim(data.valid)
dim(data.test)

# run knn with different values of k from 1 : 30
k.values <- 1:30
class.rate <- rep(0, length(k.values)) # allocate empty vector to collect correct classification rates
for (k in seq_along(k.values))
{
  pred.class <- knn(train = data.train[, -5], test=data.valid[, -5], cl = data.train[,5], k)
  class.rate[k] <- sum((pred.class==data.valid[,5]))/length(pred.class)
}

# for which value of k we reach the highest classification rate
which.max(class.rate)

# plot classification rate as a function of k
plot(class.rate, type="l", xlab="k", ylab="class. rate")

```


```{r}
# how would our model perform on the future data using the optimal k?
pred.class <- knn(train = data.train[, -5], data.test[, -5], data.train[,5], k=which.max(class.rate))
class.rate <- sum((pred.class==data.test[,5]))/length(pred.class)
print(class.rate)

```

## Classification trees

- they are often used to represent knowledge and aid decision-making
- they can be easily interpretable by anyone
- similar to knn they are assumption free and can handle various data input
- they can be presented as diagrams or pseudo-code via text
- they can be used for both classification and regression
- here we will focus on classification

**Terminology**

- **Root node**: represents the entire population of the data set
- **Splitting**: the process of dividing a node into two or more nodes
- **Decision / internal node**: when a new node is split into further nodes
- **Leaf / terminal noel**: nodes that do not split into further nodes
- **Subtree**: a subsection of a tree
- **Branch**: a subtree that is only one side of a split from a node

To make predictions we simply travel down the tree starting from the top

```{r, fig.align="center", echo=FALSE, warning=F, message=F, fig.width=10, fig.height=6, fig.cap="Example of the decision tree classifying tumour into bening and malignant type"}

library(mlbench)
library(rpart)
library(rpart.plot)

data(BreastCancer)
# dim(BreastCancer)
# levels(BreastCancer$Class)
# head(BreastCancer)
# summary(BreastCancer)

#tree.1 <- rpart(Class ~ Cl.thickness + Cell.size +  Cell.shape + Marg.adhesion, data=BreastCancer)
tree.1 <- rpart(Class ~ Cl.thickness + Cell.size +  Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli + Mitoses, data=BreastCancer)
rpart.plot(tree.1)

```

**Fitting trees**
1. pick the variable that gives the best split (often based on the lowest Gini index)
2. partition the data based on the value of this variable
3. repeat step 1. and step 2. 
4. stop splitting when no further gain can be made or some pre-set stopping rule is met
Alternatively, the data is split as much as possible and the tree is **pruned**


**Gini index**

- measures impurity in node, an alternative way of assessing model's performance to classification rates that have been shown to result in local overfitting in decision trees
- Gini index varies between 0 and (1-1/n) where $n$ is the number of categories in a dependent variable
$$Gini = \displaystyle \sum_{i=1}^{c}(p_i)^2$$ where
$c$ is the number of categories



```{r, fig.align="center", echo=FALSE, warning=F, message=F, fig.width=10, fig.height=6, fig.cap="Example of the decision tree classifying tumour into bening and malignant type with rpart() default parameteres", collapse=TRUE}
# Load BreatsCancer data and preview
data(BreastCancer)
dim(BreastCancer)
levels(BreastCancer$Class)
head(BreastCancer)

# Split data into train and test (50/50)
# (we will skip here validation data for simplicity)
set.seed(5)
n <- nrow(BreastCancer)
idx.train <- sample(c(1:n), round(n/2))
idx.test <- setdiff(c(1:n), idx.train)
data.train <- BreastCancer[idx.train, ]
data.test <- BreastCancer[idx.test,]

# Fit decision tree
# default values of rpart()
tree.1 <- rpart(Class ~ Cl.thickness + Cell.size +  Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli + Mitoses, data=BreastCancer)

# show more information about the tree
printcp(tree.1)

# plot tree
rpart.plot(tree.1)

```

**Importance of the variable**

- defined as the sum of goodness of split measures for each split for which it as the primary variable
```{r, collapse=TRUE}
# show variable.importance attribute
tree.1$variable.importance

```
**Complexity measure of a tree**

- in rpart() the complexity measure is calculated based on the size of a tree and the ability of the tree to separate the classes of the target variable
- if the next best split in growing a tree does not reduce the tree's overall complexity by a certain amount, rpart() terminates the growing process
- `cp` is the complexity parameter, set to negative amount results in a fully grown tree (maximum splits)

```{r, fig.align="center", echo=FALSE, warning=F, message=F, fig.width=10, fig.height=6, fig.cap="Example of the decision tree classifying tumour into bening and malignant type with rpart(), fully grown tree", collapse=TRUE}

# Fit decision tree
# default values of rpart()
tree.2 <- rpart(Class ~ Cl.thickness + Cell.size +  Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli + Mitoses, cp=-1, data=BreastCancer)

# show more information about the tree
printcp(tree.2)

# plot tree
rpart.plot(tree.2)

```

**Pruning a tree**

- fully grown trees do not usually perform well against data not in the training set (overfitting)
- a solution to this is to reduce (prune) the tree
- typically, this is done by choosing the complexity parameter associated with the minimum possible cross-validated error
- `xerror`, in the tree view output, in our $cp = 0.14108$ in the above case

```{r, fig.align="center", echo=FALSE, warning=F, message=F, fig.width=10, fig.height=6, fig.cap="Example of the decision tree classifying tumour into bening and malignant type with rpart() pruned tree to minimize cross-validation error", collapse=TRUE}
# prune tree
tree.2pruned <- prune(tree.2, cp = 0.14108)

# show more information about the tree
printcp(tree.2pruned)

# show variable.importance attribute
tree.2pruned$variable.importance

# plot tree
rpart.plot(tree.2pruned)
```

**Predictions of future observations**

- having our best tree model we can predict the outcome of applications on a test data set and assess model performance on "unseen" data
```{r, collapse=TRUE}
# predict cancer type given tree model
cancertype <- predict(tree.2pruned, newdata = data.test, type="class")

# cross classification table
table(cancertype, data.test$Class)

```



------

## Exercises: classification

```{exercise, "knn-rpart-repeat"}
knn and rpart practice

Make sure you can run and understand the above knn and rpart examples
```

<br />
```{exercise, "knn"}
Comparing knn() and rpart()

Given BreastCancer data

a) build a best knn() classification model that you can to predict the cancer in BreatCancer data set
b) try improving the rpart() model, look at the documentation ?rpart.control() and try to figure out and test changing other parameters, especially `minsplit` and `minbucket` 
c) compare the performance of your best knn() models with your best rpart() model on the test data 
d) share which method knn or rpart performs better together with the overall classification rate on Zulip (under Day-04)

```

```{r, collapse=TRUE}

# Install "mlbench" package
install.packages("mlbench")
library(mlbench)

# Look at the Breast Cancer data
# more about data is here: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)
data(BreastCancer)
dim(BreastCancer)
levels(BreastCancer$Class)
head(BreastCancer)

```


