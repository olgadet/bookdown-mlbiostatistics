## Precourse materials
- add more sigma / product exercises
- add Introduction to interval section (page 9, UofG-math)
- add links to external resources under each section
- make my own figure for function definition
- add inverse of a function
- add more matrix theory and exercise, especially on matrix representation of equations

### Comments from Bengt
- In 1.2, You write that “π, ω and Greek letters below are used frequently used to represent common constant,” Maybe better to write “variables, constants, functions, etc.” (consider e.g., \beta is used for regression coefficients, \Sigma for the sum, etc.)
- Also in 1.2, For Lambda you got \Gamma instead of \Lambda
- In 1.3, x^y can also denote power…; I would also write “superscript indexing” and “subscript indexing” to be clearer
- 1.6: “Indices, also known as powers” and “..in the expression x^y, x is called the base and y is called index or power” — To me, indices is something different from powers. Indices is used to indicate a specific member of a set, e.g.,  in the ordered set x=(a,b,c,d), x_2 = b. This is also true for superscript indices; if you have very complex indices, say the cell in the 3 row, 5th column of the kth matrix of X would be indexed X_{3,5}^k. NB! Since there is a great risk for confusion with the power, we (in the groups I worked in at Scilifelab) used to write X_{3,5}^{(k)}, i.e., the superscript index in parentheses (although I don’t know how common that usage is). I think you just need to avoid strictly equating indices and powers (a superscript index can be used to denote power, but all indices are not necessarily powers). Just remove “index/indices” from section1.6 and talk about power(s) instead and you’ll be fine.
- 3.1: You don’t need to change this, but possibly one could, in the figure, consider letting two elements of X map to the same element in Y. It’s pedagogical, but really not important in this context I think.]
- 3.5: Really good that you introduce ‘cases’ notation! (BTW 3.4: I’ve always thought “transcedentatl functions” is such a weird name — I just associate to the 60s and transcedental meditation:)
- Ch 4 is nice! I will bring up partial derivatives and the chain rule in my ann lecture so it’s great that they have the basis
- 6.2: inner product. Maybe one should distinguish row vertex and column vertex here: (e.g., x^T\dot y) — sort of prepares better for matrix multiplication?. Al though, I think the connection to the sum of like-indixed elements is really important — Such sums are something they will (have) run into before as well, e.g., multivariate regression.
- [6.2 and 7.5: Wouldn’t it be nice to come up with some good intuitive geometric illustration of orthogonality? But I’m so bad at that myself and it is probably far too advanced/abstract]
