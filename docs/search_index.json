[
["index.html", "Introduction to biostatistics and machine learning Preface", " Introduction to biostatistics and machine learning Olga Dethlefsen, Eva Freyhult, Bengt Sennblad, Payam Emami 2020-11-10 Preface This “bookdown” book contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees in need of biostatistical skills within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. It may also suits those already applying biostatistical methods but who have never gotten a chance to reflect on the basic statistical concepts, such as the commonly misinterpreted p-value. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/ "],
["mathematical-notations.html", "Chapter 1 Mathematical notations 1.1 Numbers 1.2 Variables, constants and letters 1.3 A precise language 1.4 Using symbols 1.5 Inequalities 1.6 Indices and powers 1.7 Exercises: notations Answers to selected exercises (notations)", " Chapter 1 Mathematical notations Aims to recapitulate the basic notations and conventions used in mathematics and statistics Learning outcomes to recognize natural numbers, integrals and real numbers to understand the differences between variables and constants to use symbols, especially Sigma and product notations, to represent mathematical operations 1.1 Numbers Natural numbers, N: numbers such as 0, 1, 3, … Integers, Z: include negative numbers …, -2, -1, 0, 1, 2 Rational numbers: numbers that can be expressed as a ratio two integers, i.e. in a form \\(\\frac{a}{b}\\), where \\(a\\) and \\(b\\) are integers, and \\(b\\neq0\\) Real numbers, R: include both rational and irrational numbers Reciprocal of any number is found by diving 1 by the number, e.g. reciprocal of 5 is \\(\\frac{1}{5}\\) Absolute value of a number can be viewed as its distance from zero, e.g. the absolute value of 6 is 6, written as \\(|6| = 6\\) and absolute value of -5 is 5, written as \\(|-5| = 5\\) Factorial of a non-negative integer number \\(n\\) is denoted by \\(n!\\) and it is a product of all positive integers less than or equal to \\(n\\), e.g. \\(4! = 4 \\cdot 3\\cdot 2 \\cdot 1 = 24\\) 1.2 Variables, constants and letters Mathematics gives us a precise language to communicate different concepts and ideas. To be able to use it it is essential to learn symbols and understand how they are used to represent physical quantities as well as understand the rules and conventions that have been developed to manipulate them. variables: things that can vary, e.g. temperature and time constants: fixed and unchanging quantities used in certain calculations, e.g. 3.14159 in principle one could freely choose letters and symbols to represent variables and constants, but it is helpful and choose letters and symbols that have meaning in a particular context. Hence, we \\(x, y, z\\), the end of the alphabet is reserved for variables \\(a, b, c\\), the beginning of the alphabet is used to represent constants \\(\\pi\\), \\(\\omega\\) and Greek letters below are used frequently used to represent common constant, e.g. \\(\\pi = 3.14159\\) Table 1.1: Uppercase and lowercase letters of the Greek alphabet Letter Upper case Lower case Letter Upper case Lower case Alpha A \\(\\alpha\\) Nu N \\(\\nu\\) Beta B \\(\\beta\\) Xi \\(\\Xi\\) \\(\\xi\\) Gamma \\(\\Gamma\\) \\(\\gamma\\) Omicron O o Delta \\(\\Delta\\) \\(\\delta\\) Pi \\(\\Pi\\) \\(\\pi\\) Epsilon E \\(\\epsilon\\) Rho P \\(\\rho\\) Zeta Z \\(\\zeta\\) Sigma \\(\\Sigma\\) \\(\\sigma\\) Eta H \\(\\eta\\) Tau T \\(\\tau\\) Theta \\(\\Theta\\) \\(\\theta\\) Upsilon Y \\(\\upsilon\\) Iota i \\(\\iota\\) Phi \\(\\Phi\\) \\(\\phi\\) Kappa K \\(\\kappa\\) Chi \\(\\Gamma\\) \\(\\gamma\\) Lambda \\(\\Gamma\\) \\(\\gamma\\) Psi \\(\\Psi\\) \\(\\psi\\) Mu M \\(\\mu\\) Omega \\(\\Omega\\) \\(\\omega\\) 1.3 A precise language Mathematics is a precise language meaning that a special attention has to be paid to the exact position of any symbol in relation to other. Given two symbols \\(x\\) and \\(y\\), \\(xy\\) and \\(x^y\\) and \\(x_y\\) can mean different things \\(xy\\) stands for multiplication, \\(x^y\\) for superscript and \\(x_y\\) for subscript 1.4 Using symbols If the letters \\(x\\) and \\(y\\) represent two numbers, then: their sum is written as \\(x + y\\) subtracting \\(y\\) from \\(x\\) is \\(x - y\\), known also as difference to multiply \\(x\\) and \\(y\\) we written as \\(x \\cdot y\\) or also with the multiplication signed omitted as \\(xy\\). The quantity is known as product of x and y multiplication is associative, e.g. when we multiply three numbers together, \\(x \\cdot y \\cdot z\\), the order of multiplication does not matter, so \\(x \\cdot y \\cdot z\\) is the same as \\(z \\cdot x \\cdot y\\) or \\(y cdot z \\cdot x\\) division is denoted by \\(\\frac{x}{y}\\) and mans that \\(x\\) is divided by \\(y\\). In this expression \\(x\\), on the top, is called numerator and \\(y\\), on the bottom, is called denominator division by 1 leaves any number unchanged, e.g. \\(\\frac{x}{1}=x\\) and division by 0 is not allowed Equal sign the equal sign \\(=\\) is used in equations, e.g. \\(x - 5 = 0\\) or \\(5x = 1\\) the equal sign \\(=\\) can be also used in formulae. Physical quantities are related through a formula in many fields, e.g. the formula \\(A=\\pi r^2\\) relates circle area \\(A\\) to its radius \\(r\\) and the formula \\(s = \\frac{d}{t}\\) defines speed as distance \\(d\\) divided by time \\(t\\) the equal sign \\(=\\) is also used in identities, expressions true for all values of the variable, e.g. \\((x-1)(x-1) = (x^2-1)\\) opposite to the equal sign is “is not equal to” sign \\(\\neq\\), e.g. we can write \\(1+2 \\neq 4\\) Sigma and Product notation the \\(\\Sigma\\) notation, read as Sigma notation, provides a convenient way of writing longs sums, e.g. the sum of \\(x_1 + x_2 + x_3 + ... + x_{20}\\) is written as \\(\\displaystyle \\sum_{i=1}^{i=20}x_i\\) the \\(\\Pi\\) notation, read as Product notation, provides a convenient way of writing longs products, e.g. \\(x_1 \\cdot x_2 \\cdot x_3 \\cdot ... \\cdot x_{20}\\) is written as \\(\\displaystyle \\prod_{i=1}^{i=20}x_i\\) 1.5 Inequalities Given any two real numbers \\(a\\) and \\(b\\) there are three mutually exclusive possibilities: \\(a &gt; b\\), meaning that \\(a\\) is greater than \\(b\\) \\(a &lt; b\\), meaning that \\(a\\) is less than \\(b\\) \\(a = b\\), meaning that \\(a\\) is equal to \\(b\\) Strict and weak inequality in \\(a &gt; b\\) and \\(a &lt; b\\) is strict as oppose to weak inequality denoted as \\(a \\ge b\\) or \\(a \\le b\\) Some useful relations are: if \\(a &gt; b\\) and \\(b &gt; c\\) then \\(a &gt; c\\) if \\(a &gt; b\\) then \\(a + c &gt; b\\) for any \\(c\\) if \\(a &gt; b\\) then \\(ac &gt; bc\\) for any positive \\(c\\) if \\(a &gt; b\\) then \\(ac &lt; bc\\) for any negative \\(c\\) 1.6 Indices and powers Indices, also known as powers are convenient when we multiply a number by itself several times e.g. \\(5 \\cdot 5 \\cdot 5\\) is written as \\(5^3\\) and \\(4 \\cdot 4 \\cdot 4 \\cdot 4 \\cdot 4\\) is written as \\(4^5\\) in the expression \\(x^y\\), \\(x\\) is called the base and \\(y\\) is called index or power The laws of indices state: \\(a^m \\cdot a^n = a^{m+n}\\) \\(\\frac{a^m}{a^n} = a^{m-n}\\) \\((a^m)^n = a^{m\\cdot n}\\) Rules derived from the laws of indices: \\(a^0 = 1\\) \\(a^1 = a\\) Negative and fractional indices: \\(a^{-m} = \\frac{1}{a^m}\\) e.g. \\(5^{-2} = \\frac{1}{5^2} = \\frac{1}{25}\\) for negative indices e.g. \\(4^{\\frac{1}{2}} = \\sqrt{4}\\) or \\(8^{\\frac{1}{3}} = \\sqrt[3]{8}\\) for fractional indices 1.7 Exercises: notations Exercise 1.1 Classify numbers as natural, integers or real. If reall, specify if they are rational or irrational. \\(\\frac{1}{3}\\) 2 \\(\\sqrt{4}\\) 2.3 \\(\\pi\\) \\(\\sqrt{5}\\) -7 0 0.25 Exercise 1.2 Classify below descriptors as variables or constants. Do you know the letters or symbols commonly used to represent these? speed of light in vacuum mass of an apple volume of an apple concentration of vitamin C in an apple distance from Stockholm central station to Uppsala central station time on the train to travel between the above stations electron charge Exercise 1.3 Write out explicitly what is meant by the following: \\(\\displaystyle \\sum_{i=1}^{i=6}k_i\\) \\(\\displaystyle \\prod_{i=1}^{i=6}k_i\\) \\(\\displaystyle \\sum_{i=1}^{i=6}i^k\\) \\(\\displaystyle \\prod_{i=1}^{i=3}i^k\\) \\(\\displaystyle \\sum_{i=1}^{n}i\\) \\(\\displaystyle \\sum_{i=1}^{i=4}(i + 1)^k\\) \\(\\displaystyle \\prod_{i=1}^{i=4}(k + 1)^i\\) \\(\\displaystyle \\prod_{i=0}^{n}i\\) Exercise 1.4 Use Sigma or Product notation to represent the long sums and products below: \\(1+2+3+4+5+6\\) \\(2^2+3^2+4^2+5^2\\) \\(4 \\cdot 5 \\cdot 6 \\cdot 7 \\cdot 8\\) \\(1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5} +...+ \\frac{1}{n}\\) \\(2-2^2+2^3-2^4 + ...+2^n\\) \\(3+6+9+12+···+60\\) \\(3x + 6x^2 + 9x^3 + 12x^4 +...+60x^{20}\\) \\(3x \\cdot 6x^2 \\cdot 9x^3 \\cdot 12x^4 \\cdot...\\cdot 60x^{20}\\) Answers to selected exercises (notations) Exr. 1.1 real, rational natural and integers, integers include natural numbers \\(\\sqrt{4} = 2\\) so it is a natural number and/subset of integers real number, rational as it could be written as \\(\\frac{23}{10}\\) real number, irrational as it cannot be explained by a simple fraction real number, irrational as it cannot be explained by a simple fraction integer, non a natural number as these do not include negative numbers natural number, although there is some argument about it as some define natural numbers as positive integers starting from 1, 2 etc. while others include 0. real, rational number, could be written as \\(\\frac{25}{100}\\) Exr. 1.2 constant, speed of light in vacuum is a constant, denoted \\(c\\) with \\(c=299 792 458 \\frac{m}{s}\\) variable, mass of an apple is a variable, different for different apple sizes, for instance 138 grams, denoted as \\(m = 100 g\\) variable, like mass volume can be different from apple to apple, denoted as \\(V\\), e.g. \\(V = 200 cm^3\\) variable, like volume and mass can vary, denoted as \\(\\rho_i\\) and defined as \\(\\rho_i=\\frac{m}{V}\\). So given 6.3 milligrams of vitamin C in our example apple, we have \\(\\rho_i=\\frac{0.0063}{2}\\frac{g}{cm^3} = 0.0000315 \\frac{g}{cm^3}\\) concentration of vitamin D constant, the distance between Stockholm and Uppsala is fixed; it could be a variable though if we were to consider an experiment on a very long time scale; distance is often denoted in physics as \\(d\\) variable, time on the train to travel between the stations varies, often denoted as \\(t\\) with speed being calculated as \\(s = \\frac{d}{t}\\) constant, electron charge is \\(e = 1.60217663\\cdot10^{-19} C\\) Exr. 1.3 \\(\\displaystyle \\sum_{i=1}^{i=6}k_i = k_1 + k_2 + k_3 + k_4 + k_5 + k_6\\) \\(\\displaystyle \\prod_{i=1}^{i=6}k_i = k_1 \\cdot k_2 \\cdot k_3 \\cdot k_4 \\cdot k_5 \\cdot k_6\\) \\(\\displaystyle \\sum_{i=1}^{i=3}i^k = 1^k + 2^k + 3^k\\) \\(\\displaystyle \\prod_{i=1}^{i=3}i^k = 1^k \\cdot 2^k \\cdot 3^k\\) \\(\\displaystyle \\sum_{i=1}^{n}i = 1 + 2 + 3 + ... + n\\) we are using dots (…) to represent all the number until \\(n\\). Here, thanks to Gauss we can also write \\(\\displaystyle \\sum_{i=1}^{n}i = \\frac{n(n+1)}{2}\\), i.e. Gauss formula for sum of first \\(n\\) natural numbers Exr. 1.4 \\(1+2+3+4+5+6 = \\displaystyle \\sum_{k=1}^{6}k\\) \\(2^2+3^2+4^2+5^2 = \\displaystyle \\sum_{x=2}^{5}x^2\\) \\(4 \\cdot 5 \\cdot 6 \\cdot 7 \\cdot 8 = \\displaystyle \\prod_{x=4}^{8}x\\) \\(1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5} + ... + \\frac{1}{n} = \\displaystyle \\sum_{k=1}^{n}\\frac{1}{k}\\) "],
["sets.html", "Chapter 2 Sets 2.1 Definitions 2.2 Basic set operations 2.3 Venn diagrams 2.4 Exercises: sets Answers to selected exercises (sets)", " Chapter 2 Sets Aims to introduce sets and basic operations on sets Learning outcomes to be able to explain what a set is to be able to construct new sets from given sets using the basic set operations to be able to use Venn diagrams to shows all possible logical relations between two and three sets 2.1 Definitions set: a well-defined collection of distinct objects, e.g. \\(S = \\{2, 4, 6\\}\\) elements: the objects that make up the set are also known as elements of the set if \\(x\\) is an element of \\(S\\), we say that \\(x\\) belongs to \\(S\\) and write \\(x \\in S\\) and if \\(x\\) is not an element of \\(S\\) we say that \\(x\\) does not belong to \\(S\\) and write \\(x \\notin S\\) a set may contain finitely many or infinitely many elements subset, \\(\\subseteq\\): if every element of set A is also in B, then A is said to be a subset of B, written as \\(A \\subseteq B\\) and pronounced A is contained in B, e.g. \\(A \\subseteq B\\), when \\(A = \\{2, 4, 6\\}\\) and $ = \\(B = \\{2, 4, 6, 8, 10\\}\\). Every set is a subset if itself. superset: for our outs \\(A\\) and \\(B\\) we can also say that \\(B\\) is a superset of \\(A\\) and write \\(B \\supset A\\) cardinality: the number of elements within a set \\(S\\), denoted as \\(|S|\\) empty set, \\(\\emptyset\\): is a unique set with no members, denoted by \\(E = \\emptyset\\) or \\(E = \\{\\}\\). The empty set is a subset of very set. 2.2 Basic set operations union of two sets, \\(\\cup\\) : two sets can be “added” together, the union of A and B, written as \\(A \\cup B\\), e.g. \\(\\{1, 2\\} \\cup \\{2, 3\\} = \\{1, 2, 3\\}\\) or \\(\\{1, 2, 3\\} \\cup \\{1, 4, 5, 6\\} = \\{1, 2, 3, 4, 5, 6\\}\\) intersection of two sets, \\(\\cap\\): a new set can be constructed by taking members of two sets that are “in common”, written as \\(A \\cap B\\), e.g. \\(\\{1, 2, 3, 4, 5, 6\\} \\cap \\{2, 3, 7\\} = \\{2, 3\\}\\) or \\(\\{1, 2, 3\\} \\cap \\{7 \\} = \\{\\emptyset \\}\\) complement of a set, \\(A&#39;\\), \\(A^c\\): are the elements not in A difference of two sets, \\(\\setminus\\): two sets can be “substracted”, denoted by \\(A \\setminus B\\), by taking all elements that are members of A but are not members of B, e.g. \\(\\{1, 2, 3, 4\\} \\setminus \\{1, 3\\} = \\{2, 4\\}\\). This is also in other words a relative complement of A with respect to B. partition of a set: a partition of a set S is a set of nonempty subset of S, such that every element x in S is in exactly one of these subsets. That is, the subset are pairwise disjoint, meaning no two sets of the partition contain elements in common, and the union of all the subset of the partition is S, e.g. Set \\(\\{1, 2, 3\\}\\) has five partitions: i) \\(\\{1\\}, \\{2\\}, \\{3\\}\\), ii) \\(\\{1, 2\\}, \\{3\\}\\), iii) \\(\\{1,3\\}, \\{2\\}\\), iv) \\(\\{1\\}, \\{2, 3\\}\\) and v) \\(\\{1,2,3\\}\\) 2.3 Venn diagrams Venn diagram is a diagram that shows all possible logical relations between a finite collection of different sets. A Venn diagrams shows elements as points in the plane, and sets as regions inside closed curves. A Venn diagram consists of multiple overlapping closed curves, usually circles, each representing a set. E.g. given \\(A = \\{1, 2, 5\\}\\) and \\(B = \\{1, 6\\}\\) Venn diagram of \\(A\\) and \\(B\\): And given \\(A = \\{1, 2, 5\\}\\), \\(B = \\{1, 6\\}\\) and \\(C= \\{4, 7\\}\\) Venn diagram of \\(A\\), \\(B\\) and \\(C\\): And given \\(A = \\{1, 2, 3, 4, 5, 6\\}\\) and \\(B= \\{2, 4, 6\\}\\) Venn diagram of \\(A\\) and \\(B\\): 2.4 Exercises: sets Exercise 2.1 Given set \\(S = \\{1, 2, 3, 4, 5, 6\\}\\): what is the subset \\(T\\) of \\(S\\) consisting of its even elements? what is the complement \\(T^c\\)? what is the subset \\(U\\) of \\(S\\) containing of the prime numbers in \\(S\\)? what is the intersection \\(T \\cap U\\)? what is the union of \\(T \\cup U\\)? what is the set difference \\(U \\setminus T\\)? Exercise 2.2 Given set \\[A = \\{cat, elephant, dog, turtle, goldfish, hamster, parrot, tiger, guinea pig, lion\\}\\] what is the subset \\(D\\) of \\(A\\) consiting of domesticated animals? what is the subset \\(C\\) of \\(A\\) consiting of Felidae (cat) family? what is the interection of \\(D\\) and \\(C\\)? what is the complement of \\(D\\), \\(D^c\\)? what is the union of \\(D\\) and \\(C\\)? what is the set difference of \\(A \\setminus C\\)? can you draw Venn diagram showing relationship between \\(D\\) and \\(C\\)? Answers to selected exercises (sets) Exr. 2.1 \\(T = \\{2, 4, 6\\}\\) \\(T^c = \\{1, 3, 5\\}\\), i.e. \\(T^c\\) contains all the elements of \\(S\\) not in \\(T\\) \\(U = \\{2, 3, 5\\}\\), the primes in \\(S\\) \\(T \\cap U = \\{2\\}\\), common elements of \\(T\\) and \\(U\\), i.e. the even and prime numbers \\(T \\cup U = \\{2, 3, 4, 5, 6\\}\\) \\(U \\setminus T = \\{3, 5\\}\\), consisting of the elements of \\(U\\) that are not in \\(T\\) "],
["functions.html", "Chapter 3 Functions 3.1 Definitions 3.2 Evaluating function 3.3 Plotting function 3.4 Standard classes of functions 3.5 Piecewise functions 3.6 Exercises: functions Answers to selected exercises (functions)", " Chapter 3 Functions Aims to revisit the concept of a function Learning outcomes to be able to explain what function, function domain and function range are to be able to identify input, output, argument, independent variable, dependent variable to be able to evaluate function for a given value and plot the function 3.1 Definitions Figure 3.1: Formal function defition A function, \\(f(x)\\), can be viewed as a rule that relates input \\(x\\) to an output \\(f(x)\\) In order for a rule to be a function it must produce a single output for any given input Input \\(x\\) is also known as argument of the function Domain of a function: the set of all values that the function “maps” Range: the set of all values that the function maps into Many names are used interchangeably Functions have been around for a while and there are many alternative names and writing conventions are being used. Common terms worth knowing: Figure 3.2: Common function terms 3.2 Evaluating function To evaluate a function is to replace (substitute) its variable with a given number or expression. E.g. given a rule (function) that maps temperature measurements from Celsius to Fahrenheit scale: \\[f(x) = 1.8x + 32\\] where \\(x\\) is temperature measurements in Celsius and \\(f(x)\\) is the associated value in Fahrenheit, we can find for a given temperature in Celsius corresponding temperature in Fahrenheit. Let’s say we measure 10 Celsius degrees one autumn day in Uppsala and we want to share this information with a friend in USA. We can find the equivalent temperature in Fahrenheit by evaluating our function at 10, \\(f(10)\\), giving us \\[f(10) = 1.8\\cdot 10 + 32 = 50\\] 3.3 Plotting function Function graphs are a convenient way of showing functions, by looking at the graph it is easier to notice function’s properties, e.g. for which input values functions yields positive outcomes or whether the function is increasing or decreasing. To graph a function, one can start by evaluating function at different values for the argument \\(x\\) obtaining \\(f(x)\\), plotting the points by plotting the pairs \\((x, f(x))\\) and connecting the dots. E.g. evaluating our above temperature rule at -20, -10, 0, 10, 20, 30 Celsius degrees results in: x (Celsius degrees) evaluates f(x) (Farenheit degress) -20 \\(f(-20) = 1.8 \\cdot (-20) + 32\\) -4 -10 \\(f(-20) = 1.8 \\cdot (-10) + 32\\) 14 0 \\(f(-20) = 1.8 \\cdot (0) + 32\\) 32 10 \\(f(-20) = 1.8 \\cdot (10) + 32\\) 50 20 \\(f(-20) = 1.8 \\cdot (20) + 32\\) 68 20 \\(f(-20) = 1.8 \\cdot (30) + 32\\) 86 Figure 3.3: Graph of f(x) for the temeprature rule 3.4 Standard classes of functions Algebraic function: functions that can be expressed as the solution of a polynomial equation with integer coefficients, e.g. constant function \\(f(x) = a\\) identity function \\(f(x) = x\\) linear function \\(f(x) = ax + b\\) quadratic function \\(f(x) = a + bx + cx^2\\) cubic function \\(fx() = a + bx + cx^2 + dx^3\\) Transcedental functions: functions that are not algebraic, e.g. exponential function \\(f(x) = e^x\\) logarithimic function \\(f(x) = log(x)\\) trigonometric function \\(f(x) = -3sin(2x)\\) Figure 3.4: Examples of the standard classess of functions 3.5 Piecewise functions A function can be in pieces, i.e. we can create functions that behave differently based on the input \\(x\\) value. They are useful to describe situations in w which a rule changes as the input value crosses certain “boundaries”. E.g. a function value could be fixed in a given range and equal to the input value (identify function) for input values outside this range \\[\\begin{equation} f(x) = \\left\\{ \\begin{array}{cc} 2 &amp; \\mathrm{if\\ } x \\le 1 \\\\ x &amp; \\mathrm{if\\ } x&gt;1 \\\\ \\end{array} \\right. \\end{equation}\\] The function can be split in many pieces, e.g. the personal training fee in SEK may depend whether the personal trainer is hired for an hour, two hours or three or more hours: \\[\\begin{equation} f(h) = \\left\\{ \\begin{array}{cc} 500 &amp; \\mathrm{if\\ } h \\le 1 \\\\ 750 &amp; \\mathrm{if\\ } 1 &lt; h \\le 2 \\\\ 500 + 250 \\cdot h &amp; \\mathrm{if\\ } h &gt; 2 \\\\ \\end{array} \\right. \\end{equation}\\] Figure 3.5: Examples of piece-wise functions 3.6 Exercises: functions Exercise 3.1 Given the function for the personal trainer costs: \\[\\begin{equation} f(h) = \\left\\{ \\begin{array}{cc} 500 &amp; \\mathrm{if\\ } h \\le 1 \\\\ 750 &amp; \\mathrm{if\\ } 1 &lt; h \\le 2 \\\\ 500 + 250 \\cdot h &amp; \\mathrm{if\\ } h &gt; 2 \\\\ \\end{array} \\right. \\end{equation}\\] How much would you pay for a 4-hours session? Evaluate function f(h) for value 4. for a 2-hour session? Evalue function f(h) for value 2. Exercise 3.2 A museum charges 50 SEK per person for a guided tour with a group of 1 to 9 people or a fixed 500 SEK fee for a group of 10 or more people. Write a function relating the number of people \\(n\\) to the cost \\(C\\). Exercise 3.3 Given function \\[\\begin{equation} f(x) = \\left\\{ \\begin{array}{cc} x^2 &amp; \\mathrm{if\\ } x \\le 1 \\\\ 3 &amp; \\mathrm{if\\ } 1 &lt; x \\le 2 \\\\ x &amp; \\mathrm{if\\ } x &gt; 2 \\\\ \\end{array} \\right. \\end{equation}\\] sketch a graph of a function for \\(x \\in (-4, 4)\\), i.e.. for \\(x\\) between -4 and 4 evaluate function at f(1) evaluate function at f(4) Answers to selected exercises (functions) Exr. 3.1 \\(f(4) = 500 + 250 \\cdot 4 = 1500\\) \\(f(2) = 750\\) as \\(h \\le 2\\) means less or equal to 2, that is including 2 "],
["differentiation.html", "Chapter 4 Differentiation 4.1 Rate of change 4.2 Average rate of change across an interval 4.3 Rate of change at a point 4.4 Terminology and notation 4.5 Table of derivatives 4.6 Exercises (differentiation) Answers to selected exercises (differentiation)", " Chapter 4 Differentiation Aims introduce the concept of differentiation and rules of differentiation Learning outcomes to be able to explain differentiation in terms of rate of change to be able to find derivatives in simple cases 4.1 Rate of change We are often interested in the rate at which some variable is changing, e.g. we may be interested in the rate at which the temperature is changing or the rate of water levels increasing Rapid or unusual changes may indicate that we are dealing with unusual situations, e.g. global warming or a flood Rates of change can be positive, negative or zero corresponding to a variable increasing, decreasing and non-changing Figure 4.1: The function \\(f(x)\\) changes at different rates for different values of \\(x\\) The function \\(f(x) = x^4 - 4x^3 - x^2 - e^{-x}\\) changes at different rates for different values of \\(x\\), e.g. between \\(x \\in (-10, -9)\\) the \\(f(x)\\) is increasing at slightly higher pace than \\(x \\in (5,6)\\) between \\(x \\in (-7, -5)\\) the \\(f(x)\\) is decreasing and between \\(x \\in (0, 1)\\) the \\(f(x)\\) is not changing to be able to talk more precisely about the rate of change than just saying “large and positive” or “small and negative” change we need to quantify the changes, i.e. assign the rate of change an exact value Differentiation is a technique for calculating the rate of change of any function 4.2 Average rate of change across an interval Figure 4.2: The average rate of change of \\(f(x)\\) with respect to \\(x\\) over \\([a, b]\\) is equal to the slope of the secant line (in black) To dive further into calculating the rate of change let’s look at Figure 4.2 and define the average rate of change of a function across an interval. Figure 4.2 shows a function \\(f(x)\\) with two possible argument values \\(a\\) and \\(b\\) marked and their corresponding function values \\(f(a)\\) and \\(f(b)\\). Consider that \\(x\\) is increasing from \\(a\\) to \\(b\\). The change in \\(x\\) is \\(b-a\\), i.e. as \\(x\\) increases from \\(a\\) to \\(b\\) the function \\(f(x)\\) increase from \\(f(a)\\) to \\(f(b)\\). The change in \\(f(x)\\) is \\(f(b)-f(a)\\) and the average rate of change of \\(y\\) across the \\([a,b]\\) interval is: \\[\\begin{equation} \\frac{change\\:in\\:y}{change\\:in\\:x}=\\frac{f(b)-f(a)}{b-a} \\tag{4.1} \\end{equation}\\] E.g. let’s take a quadratic function \\(f(x)=x^2\\) and calculate the average rate of change across the interval \\([1, 4]\\). The change in \\(x\\) is \\(4-1\\) and the change in \\(f(x)\\) is \\(f(4) - f(1) = 4^2 -1^2 = 16 - 1 = 15\\). So the average rate of change is \\(\\frac{15}{3}=5\\). What does this mean? It means that across the interval \\([1,4]\\) on average the \\(f(x)\\) value increases by 5 for every 1 unit increase in \\(x\\). If we were to look at the average rate of change across the interval \\([-2, 0]\\) we would get \\(\\frac{f(0)-f(-2)}{0 - (-2)}=\\frac{0 - (-2)^2}{2}=\\frac{-4}{2} = -2\\). Here, over the \\([-2, 0]\\) on average the \\(f(x)\\) value decreases by 2 for every 1 unit increase in \\(x\\). Looking at the graph of \\(f(x)=x^2\\) verifies our calculations Figure 4.3: Example function \\(f(x) = x^2\\) 4.3 Rate of change at a point We often need to know the rate of change of a function at a point, and not simply an average rate of change across an interval. Figure 4.4, similar to Figure 4.2, shows, instead of two points \\(a\\) and \\(b\\), point \\(a\\) and a second point defined in terms of its distance from the first point \\(a\\). Thus, the two points are now \\(a\\) and \\(a + h\\) and the distance between the two points is equal to \\(h\\). Now we can write that: \\[\\frac{change\\:in\\:y}{change\\:in\\:x}=\\frac{f(a+h)-f(a)}{a+h-a} = \\frac{f(a+h)-f(a)}{h}\\] Figure 4.4: The average rate of change of \\(f(x)\\) with respect to \\(x\\) over \\([a, b]\\) is equal to the slope of the secant line (in black) Further: if we assume that the second point \\(a+h\\) is really close to \\(a\\), meaning that \\(h\\) approaches 0, denoted as \\(h \\rightarrow 0\\), we can find the rate of change at the point \\(a\\) the distance between the two points \\(a\\) and \\(a+h\\) is getting smaller and so is the difference of the function values \\(f(a+h) - f(a)\\). We denote these small differences as \\(\\delta x\\) and \\(\\delta y\\), pronounced “delta x” and “delta y”, respectively. the term \\(\\delta\\) reads as “delta” and represents a small change We can thus continue and write that a rate of change of a function at a point is given by \\[\\begin{equation} \\frac{small\\:change\\:in\\:y}{small\\:change\\:in\\:x} = \\lim_{h\\to0}\\frac{f(a+h)-f(a)}{h} \\tag{4.1} \\end{equation}\\] E.g. let’s look at the linear function \\(f(x) = 2x+3\\). We can find the rate of change at any point of \\(x\\) by: \\[\\frac{small\\:change\\:in\\:y}{small\\:change\\:in\\:x} = \\\\\\frac{f(x+h)-f(x)}{x+h-x}= \\lim_{h\\to0}\\frac{2(x+h)+3-(2x+3)}{x+h-x}=\\lim_{h\\to0}\\frac{2h}{h}=2\\] It means that the function value \\(f(x)\\) increases by 2 for every small increase, \\(h\\), in \\(x\\). Here, this increase is the same for all the values of \\(x\\), i.e. it does not depend on \\(x\\). The change in function value \\(f(x)\\) can depend on the value of \\(x\\), for instance if we look at the quadratic \\(f(x)=x^2\\) function, we get: \\[\\frac{small\\:change\\:in\\:y}{small\\:change\\:in\\:x} = \\\\ \\frac{f(x+h)-f(x)}{x+h-x}=\\lim_{h\\to0}\\frac{x^2+2xh+h^2-x^2}{h}=\\lim_{h\\to0}\\frac{2xh+h^2}{h}=2x+h\\] This means that: the rate of change for the function \\(f(x)\\) at a point \\(x\\) is \\(2x\\) the \\(f(x)\\) value increases by \\(2x\\) for every small increase, \\(h\\), in \\(x\\) the rate of change along a quadratic function is changing constantly according to the value of \\(x\\) we are looking at, it is a function of \\(x\\) and finally that the rate of change does not give us any information about the rate of change globally. 4.4 Terminology and notation differentiation is the process of finding the rate of change of a given function the function is said to be differentiated the rate of change of a function is also known as the derivative of the function given a function \\(f(x)\\) we say that we differentiate function in respect to \\(x\\) and write: \\[\\lim_{h\\to0}\\frac{\\delta y}{\\delta x}= \\frac{dy}{dx}\\] or use the “prime” \\[f´(x)\\] 4.5 Table of derivatives in practice, there is no need to compute \\(\\displaystyle \\lim_{h\\to0}\\frac{\\delta y}{\\delta x}\\) every time when we want to find a derivative of a function instead, we can use patterns of the common functions and their derivatives Table 4.1: Common functions and their derivatives, \\(k\\) denotes a constant Function \\(f(x)\\) Derivative \\(f&#39;(x)\\) \\(k\\) \\(0\\) \\(x\\) \\(1\\) \\(kx\\) \\(k\\) \\(x^n\\) \\(nx^{n-1}\\) \\(kx^n\\) \\(knx^{n-1}\\) \\(e^x\\) \\(e^x\\) \\(e^{kx}\\) \\(ke^{kx}\\) \\(\\ln(x)\\) \\(\\frac{1}{x}\\) \\(\\ln(kx)\\) \\(\\frac{1}{x}\\) We can use the Table 4.1 to find derivatives of some of the functions e.g. \\(f(x) = 3x\\), \\(f&#39;(x) = 3\\) \\(f(x) = 2x^4\\), \\(f&#39;(x) = 2*4x^{4-1} = 8x^3\\) \\(f(x) = e^{2x}\\), \\(f&#39;(x) = 2e^{2x}\\) \\(f(x) = \\ln(4x)\\), \\(f&#39;(x) = \\frac{4}{x}\\) 4.6 Exercises (differentiation) Exercise 4.1 # Find derivatives of the functions \\(f(x) = 2\\) \\(f(x) = 2x + 1\\) \\(f(x) = 5x^2\\) \\(f(x) = 4x^3 + x^2\\) \\(f(x) = \\sqrt(x)\\) \\(f(x) = \\ln(2x)\\) \\(f(x) = e^{x}\\) \\(f(x) = \\frac{9}{x^2} + ln(4x)\\) \\(f(x) = 4x−6x^6\\) \\(f(x) = \\frac{3}{x^2}\\) Answers to selected exercises (differentiation) Exr. 4.1 \\(f(x) = 2\\), \\(f&#39;(x) = 0\\) \\(f(x) = 2x + 1\\), \\(f&#39;(x) = 2\\) \\(f(x) = 5x^2\\), \\(f&#39;(x)= 10x\\) \\(f(x) = 4x^3 + x^2\\), \\(f&#39;(x)=12x^2 + 2x\\) \\(f(x) = \\sqrt(x) = x^{\\frac{1}{2}}\\), \\(f&#39;(x)=\\frac{1}{2}x^{\\frac{1}{2}-1} = \\frac{1}{2}x^{-\\frac{1}{2}}\\) \\(f(x) = \\ln(2x)\\), \\(f&#39;(x) = \\frac{1}{x}\\) \\(f(x) = e^{x}\\), \\(f&#39;(x) = e^x\\) "],
["integration.html", "Chapter 5 Integration 5.1 Reverse to differentiation 5.2 What is constant of integration? 5.3 Table of integrals 5.4 Definite integrals Answers to selected exercises (integration)", " Chapter 5 Integration Aims to introduce the concept of integration Learning outcomes to be able to explain what integration is to be able to explain the relationship between differentiation and integration to be able to integrate simple functions to to able to use integration to calculate the area under the curve in simple cases 5.1 Reverse to differentiation when a function \\(f(x)\\) is known we can differentiate it to obtain the derivative \\(f&#39;(x)\\) the reverse process is to obtain \\(f(x)\\) from the derivative this process is called integration apart from simple reversing differentiation integration comes very useful in finding areas under curves, i.e. the area above the x-axis and below the graph of \\(f(x)\\), assuming that \\(f(x)\\) is positive the symbol for integration is \\(\\int\\) and is known as “integral sign” E.g. let’s take a function \\(f(x) = x^2\\). Suppose we only have a derivative, which is \\(f&#39;(x) = 2x\\) and we would like to find the function given this derivative. Formally we write: \\[\\int 2x dx = x^2 +c\\] where: the term \\(2x\\) within the integral is called the integrand the term \\(dx\\) indicates the name of the variable involved, here \\(x\\) \\(c\\) is constant of integration 5.2 What is constant of integration? Integration reverses the process of differentiation, here, given our example function \\(f(x) = x^2\\) that we pretended we do not know, we started with the derivative \\(f´(x) = 2x\\) and via integration we obtained back the very function \\[\\int 2x dx = x^2\\] However, many function can result in the very same derivative since the derivative of a constant is 0 e.g. a derivatives of \\(f(x) = x^2\\), \\(f(x) = x^2 + 10\\) and \\(f(x) = x^2 + \\frac{1}{2}\\) all equal to \\(f&#39;(x) = 2x\\) We have to take this into account when we are integrating, i.e. reverting differentiation. As we have no way of knowing what the original function constant is, we add it in form of \\(c\\), i.e. unknown constant, called the constant of integration. 5.3 Table of integrals Similar to differentiation, in practice we can use tables of integrals to be able to find integrals in simple cases Table 5.1: Common functions and their integrals, \\(k\\) denotes a constant Function \\(f(x)\\) Integral \\(\\int f(x) dx\\) \\(constant\\:k\\) \\(kx + c\\) \\(x\\) \\(\\frac{x^2}{2}+c\\) \\(kx\\) \\(k\\frac{x^2}{2}+c\\) \\(x^n\\) \\(\\frac{x^{n+1}}{n+1}+c\\;\\; if\\;n\\neq-1\\) \\(kx^n\\) \\(k\\frac{x^{n+1}}{n+1}+c\\) \\(e^x\\) \\(e^x+c\\) \\(e^{kx}\\) \\(\\frac{e^{kx}}{k}+c\\) \\(\\frac{1}{x}\\) \\(\\ln(x)+c\\) E.g. \\(\\int 4x^3 dx = \\frac{4x^{3+1}}{3+1}=x^4 + c\\) \\(\\int (x^2 + x) dx = \\frac{x^3}{3} + \\frac{x^2}{2} +c\\) (note: we can evaluate integrals separately and add them as integration as differentiation is linear) 5.4 Definite integrals the above examples of integrals are indefinite integrals, the result of finding an indefinite integral is usually a function plus a constant of integration we have also definite integrals, so called because the result is a definite answer, usually a number, with no constant of integration definite integrals are often used to areas bounded by curves or, as we will cover later on, estimating probabilities we write: \\[\\int_{a}^bf(x)dx\\] where: \\(\\int_{a}^bf(x)dx\\) is called the definite integral of \\(f(x)\\) from \\(a\\) to \\(b\\) the numbers \\(a\\) and \\(b\\) are known as lower and upper limits of the integral E.g. let’s look at the function \\(f(x) = x\\) plotted below and calculate a definite integral from \\(0\\) to \\(2\\). Figure 5.1: Graph of function \\(f(x) = x\\) We write \\[\\int_{0}^2f(x)dx = \\int_{0}^2 xdx = \\Bigr[ \\frac{1}{2}x^2\\Bigr]_0^2 = \\frac{1}{2}(2)^2 - \\frac{1}{2}(0)^2 = 2\\] so first find the integral and then we evaluate it at upper limit and subtracting the evaluation at the lower limit. Here, the result it 2. What would be the result if you tried to calculate the triangle area on the above plot, area defined by the blue vertical lines drawn at 0 and 2 and horizontal x-axis? The formula for the triangle area is \\(Area = \\frac{1}{2}\\cdot base \\cdot height\\) so here \\(Area = \\frac{1}{2} \\cdot 2 \\cdot 2 = 2\\) the same result as achieved with integration. Exercise 5.1 Integrate: \\(\\int 2 \\cdot dx\\) \\(\\int 2x\\cdot dx\\) \\(\\int (x^4 + x^2 + 1)\\cdot dx\\) \\(\\int e^x\\cdot dx\\) \\(\\int e^{2x}\\cdot dx\\) \\(\\int \\frac{2}{x}\\cdot dx\\) \\(\\int_2^4 2x\\cdot dx\\) \\(\\int_0^4 (x^2+1)dx\\) \\(\\int (x^4 + \\frac{2}{x} + e^{2x}) dx\\) \\(\\int_0^4 (x^4+1) dx\\) Answers to selected exercises (integration) Exr. 5.1 \\(\\int 2 \\cdot dx = 2x +c\\) \\(\\int 2x\\cdot dx = \\frac{2x^2}{2} = x^2 + c\\) \\(\\int (x^4 + x^2 + 1)\\cdot dx = \\frac{x^5}{5} + \\frac{x^3}{3} + x + c\\) \\(\\int e^x\\cdot dx = e^x + c\\) \\(\\int e^{2x}\\cdot dx = \\frac{1}{2}e^{2x}\\) \\(\\int \\frac{2}{x}\\cdot dx =\\int 2\\cdot \\frac{1}{x}\\cdot dx = 2 \\ln{x}+ c\\) \\(\\int_2^4 2x\\cdot dx = \\Bigr[x^2\\Bigr]_2^4 = 16 - 4 = 12\\) \\(\\int_0^4 (x^2+1)dx = \\Bigr[\\frac{x^3}{3} + x \\Bigr]_0^4=\\frac{4^3}{3}+4 - 0 = \\frac{64}{3}+4 = \\frac{76}{3}\\) "],
["vectors.html", "Chapter 6 Vectors 6.1 Vectors 6.2 Operations on vectors 6.3 Null and unit vector Answers to selected exercises (vectors and matrices)", " Chapter 6 Vectors Aims to introduce vectors and basic vectors operations Learning outcomes to be able to write \\(n\\)-dimensional vectors using vector notations to be able to perform addition and scalar multiplication to be able to check if two vectors are orthogonal A large number of statistical models use vectors and matrices, both for compact representations, and for the calculations, e.g. parameter estimates. 6.1 Vectors A vector is an ordered set of number These numbers, e.g. in vector \\(\\mathbf{x}\\) can be expressed as a row \\(\\mathbf{x}=[6\\quad 0\\quad 5 \\dots1]\\) or as a column \\(\\mathbf{x}=\\begin{bmatrix} 6 \\\\ 0 \\\\ 5 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\) the number of elements in a vector is referred to as its dimension and we often use \\(n\\) to express \\(n\\)-dimensional vector, where \\(n\\) can be any natural number here, we denote vectors using small bold font \\(\\mathbf{x}\\), other notations may include an arrow \\(\\vec x\\) or overline \\(\\overline{x}\\) also parentheses are used interchangeably with square bracket, e.g. \\(\\mathbf{x}=[6\\quad 0\\quad 5 \\dots1]\\) can be written as \\(\\mathbf{x}=(6\\quad 0\\quad 5 \\dots1)\\) or \\(\\begin{pmatrix} 6\\\\ 0\\\\ 5\\\\ \\vdots \\\\ 1 \\end{pmatrix}\\) 6.2 Operations on vectors Given two vectors of the same dimension: \\(\\mathbf{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\) and \\(\\mathbf{y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\) Addition: we add two vectors, element by element \\(\\mathbf{x} + \\mathbf{y}=\\begin{bmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ x_3 + y_3 \\\\ \\vdots \\\\ x_n + y_n \\end{bmatrix}\\) Scalar multiplication: we can multiple vector by a numerical value, scalar, denoted as \\(\\gamma\\): \\[\\gamma \\cdot \\mathbf{x} =\\begin{bmatrix} \\gamma \\cdot x_1 \\\\ \\gamma \\cdot x_2 \\\\ \\gamma \\cdot x_3 \\\\ \\vdots \\\\ \\gamma \\cdot x_n \\end{bmatrix}\\] Difference \\(\\mathbf{x} - \\mathbf{y}\\) can be written as \\(\\mathbf{x} + (-1) \\cdot \\mathbf{y}\\), thus we multiply second vector with \\(-1\\) and then add two vectors Linear combination of vectors: the vector \\(\\gamma \\cdot \\mathbf{x} + \\delta \\cdot \\mathbf{y}\\) is said to be a linear combination of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\): \\[\\gamma \\cdot \\mathbf{x} + \\delta \\cdot \\mathbf{y} =\\begin{bmatrix} \\gamma \\cdot x_1 + \\delta \\cdot y_1 \\\\ \\gamma \\cdot x_2 + \\delta \\cdot y_2\\\\ \\gamma \\cdot x_3 + \\delta \\cdot y_3\\\\ \\vdots \\\\ \\gamma \\cdot x_n + \\delta \\cdot y_n \\end{bmatrix}\\] Inner product of vectors is given by: \\[\\mathbf{x} \\cdot \\mathbf{y} = x_1 \\cdot y_1 + x_2 \\cdot y_2 + \\dots x_n \\cdot y_n = \\displaystyle\\sum_{i=1}^{n}x_i\\cdot y_i\\] Orthogonality of vectors: two vectors are said to be orthogonal if their inner product is zero \\[\\mathbf{x} \\cdot \\mathbf{y} =\\displaystyle\\sum_{i=1}^{n}x_i\\cdot y_i = 0\\] 6.3 Null and unit vector a null vector is a vector whose elements are all \\(0\\); the difference between any vector and itself yields a null vector a unit vector is a vector whose elements are all \\(1\\) Exercise 6.1 Based on vector definitions and operations: find the vector \\(\\mathbf{x} + \\mathbf{y}\\) when \\(\\mathbf{x} =\\begin{bmatrix} 1 \\\\ 2 \\\\ 5 \\end{bmatrix}\\) and \\(\\mathbf{y} =\\begin{bmatrix} 0\\\\ 3 \\\\ 1 \\end{bmatrix}\\) find the vector \\(2\\mathbf{x} - \\mathbf{y}\\) when \\(\\mathbf{x} =\\begin{bmatrix} -2 \\\\ 3 \\\\ 5 \\end{bmatrix}\\) and \\(\\mathbf{y} =\\begin{bmatrix} 0\\\\ -4 \\\\ 7 \\end{bmatrix}\\) are \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) vectors orthogonal when when \\(\\mathbf{u} =\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \\(\\mathbf{v} =\\begin{bmatrix} 2\\\\ -1 \\end{bmatrix}\\)? are \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) vectors orthogonal when when \\(\\mathbf{u} =\\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}\\) and \\(\\mathbf{v} =\\begin{bmatrix} 7\\\\ 5 \\end{bmatrix}\\)? find the value \\(n\\) such that the vectors \\(\\mathbf{u} =\\begin{bmatrix} 2 \\\\ 4 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{v} =\\begin{bmatrix} n\\\\ 1 \\\\ 8 \\end{bmatrix}\\) are orthogonal. Answers to selected exercises (vectors and matrices) Exr. 6.1 \\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 5 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 3 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 + 0\\\\ 2 + 3 \\\\ 5 + 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 5 \\\\ 6 \\end{bmatrix}\\] \\[2\\mathbf{x} - \\mathbf{y} = \\begin{bmatrix} 2 \\cdot (-2) \\\\ 2 \\cdot 3 \\\\ 2 \\cdot 5 \\end{bmatrix} + \\begin{bmatrix} (-1) \\cdot 0 \\\\ (-1) \\cdot (-4) \\\\ (-1) \\cdot 7 \\end{bmatrix} = \\begin{bmatrix} -4 \\\\ 6 \\\\ 10 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 4 \\\\ -7 \\end{bmatrix} = \\begin{bmatrix} -4 + 0 \\\\ 6 + 4 \\\\ 10 - 7 \\end{bmatrix} = \\begin{bmatrix} -4 \\\\ 10 \\\\ 3 \\end{bmatrix}\\] Yes, to check orthogonality we need to calculate the inner product of two vectors and see if it is equal to 0, here \\(\\mathbf{u} \\cdot \\mathbf{v} =\\displaystyle\\sum_{i=1}^{2}u_i\\cdot v_i = 1 \\cdot 2 + 2 \\cdot (-1) = 2 - 2 = 0\\) No, since the inner product does not equal to 0 \\[\\mathbf{u} \\cdot \\mathbf{v} =\\displaystyle\\sum_{i=1}^{2}u_i\\cdot v_i = 3 \\cdot 7 + (-1) \\cdot 5 = 21 - 5 = 16 \\neq 0\\] "],
["matrices.html", "Chapter 7 Matrices 7.1 Matrix 7.2 Special matrices 7.3 Matrix operations 7.4 Inverse of a matrix 7.5 Orthogonal matrix Answers to selected exercises (matrices)", " Chapter 7 Matrices Aims to introduce matrix and basic matrices operations Learning outcomes to be able to write matrices using matrix notations to be able to perform simple matrix operations such as adding and multiplication to be able to find the reverse of the 2-dimensional matrix 7.1 Matrix A matrix is a rectangular array of numbers e.g. \\[\\mathbf{A}=\\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; \\dots &amp; x_{1n} \\\\ x_{21} &amp; x_{22} &amp; x_{23} &amp; \\dots &amp; x_{2n} \\\\ \\dots &amp; \\dots &amp; \\dots&amp; \\dots &amp; \\dots\\\\ x_{m1} &amp; x_{m2} &amp; x_{1m3} &amp; \\dots &amp; x_{mn} \\\\ \\end{bmatrix}\\] where: the notional subscripts in the typical element \\(x_{ij}\\) refers to its row and column location in the array, e.g. \\(x_{12}\\) refers to element in the first row and second column we say that matrix has \\(m\\) rows and \\(n\\) columns and the dimension of a matrix is defined as \\(m \\times n\\) a matrix can be viewed as a set of column vectors or a set of row vectors a vector can be viewed as a matrix with only one column or with only one row 7.2 Special matrices A matrix with the same number of rows as columns, \\(m = n\\), is said to be a square matrix A matrix that is not squared, \\(m \\neq n\\) is called rectangular matrix A null matrix is composed of all 0 An identity matrix, denoted as \\(\\mathbf{I}\\) or \\(\\mathbf{I_n}\\), is a square matrix with 1’s on the main diagonal and all other elements equal to 0, e.g. a three-dimensional identity matrix is \\[\\mathbf{I}=\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] A square matrix is said to be symmetric if \\(x_{ij} = x_{ji}\\) e.g. \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 4 &amp; 2 \\\\ 4 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\end{bmatrix}\\] A diagonal matrix is a square matrix whose non-diagonal entries are all zero, that is \\(x_{ij} = 0\\) for \\(i \\neq j\\), e.g. \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix}\\] An upper-triangular matrix is a square matrix in which all entries below the diagonal are 0, that is \\(x_{ij}=0\\) for \\(i&lt;j\\) e.g. \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 3 &amp; 4 \\\\ 0 &amp; 2 &amp; 5 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix}\\] A lower-triangular matrix is a square matrix in which all entries above the digonal are 0, that is hat is \\(x_{ij}=0\\) for \\(i&gt;j\\) e.g. \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\] 7.3 Matrix operations matrix \\(\\mathbf{A} = \\mathbf{B}\\) if both matrices have exactly the same dimension and if each element of \\(\\mathbf{A}\\) equals to the corresponding element of e.g. \\(\\mathbf{A} = \\mathbf{B}\\) if \\(\\mathbf{A}=\\begin{bmatrix} 1 &amp; 3 &amp; 4 \\\\ 0 &amp; 2 &amp; 5 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix}\\) and \\(\\mathbf{B}=\\begin{bmatrix} 1 &amp; 3 &amp; 4 \\\\ 0 &amp; 2 &amp; 5 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix}\\) for any matrix \\(\\mathbf{A}\\) the transpose, denoted by \\(\\mathbf{A}^\\top\\) or \\(\\mathbf{A}^\\prime\\), is obtained by interchanging rows and columns, e.g. given matrix \\(\\mathbf{A}=\\begin{bmatrix} 1 &amp; 3 &amp; 4 \\\\ 0 &amp; 2 &amp; 5 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix}\\) we have \\(\\mathbf{A}^\\top=\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 3 &amp; 2 &amp; 0 \\\\ 4 &amp; 5 &amp; 3 \\end{bmatrix}\\). The transpose of a transpose of a matrix yield the original matrix, \\(\\Big(\\mathbf{A}^\\top\\Big)^\\top = \\mathbf{A}\\) we can add two matrices if they have the same dimension, e.g. \\[\\mathbf{A} + \\mathbf{B} = \\mathbf{A} =\\begin{bmatrix} x_{11} &amp; x_{12} \\\\ x_{21} &amp; x_{22} \\end{bmatrix} + \\begin{bmatrix} y_{11} &amp; y_{12} \\\\ y_{21} &amp; y_{22} \\end{bmatrix} = \\begin{bmatrix} x_{11}+y_{11} &amp; x_{12}+y_{12} \\\\ x_{21}+y_{21} &amp; x_{22}+y_{22} \\end{bmatrix}\\] we can multiply a matrix by a scalar \\(\\delta\\) e.g. \\[\\delta \\cdot \\mathbf{A} = \\begin{bmatrix} \\delta \\cdot x_{11} &amp; \\delta \\cdot x_{12} \\\\ \\delta \\cdot x_{21} &amp; \\delta \\cdot x_{22} \\end{bmatrix}\\] we can multiply two matrices if they are conformable, i.e. first matrix has the same number of columns as the number of rows in the second matrix. We then can write: \\[\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\end{bmatrix} + \\begin{bmatrix} y_{11} &amp; y_{12} \\\\ y_{21} &amp; y_{22} \\\\ y_{31} &amp; y_{32} \\end{bmatrix} = \\\\\\ \\begin{bmatrix} x_{11} \\cdot y_{11} + x_{12} \\cdot y_{21} + x_{13} \\cdot y_{31} &amp; x_{11} \\cdot y_{12} + x_{12} \\cdot y_{22} + x_{13} \\cdot y_{32} \\\\ x_{21} \\cdot y_{22} + x_{23} \\cdot y_{21} + x_{13} \\cdot y_{31} &amp; x_{21} \\cdot y_{12} + x_{23} \\cdot y_{22} + x_{13} \\cdot y_{32} \\end{bmatrix}\\] 7.4 Inverse of a matrix For a square matrix \\(\\mathbf{A}\\) there may exist a matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{A} \\cdot \\mathbf{B} = \\mathbf{I}\\). An inverse, if it exists, is denoted as \\(\\mathbf{A}^{-1}\\) and we can rewrite the definition as \\[\\mathbf{A} \\cdot \\mathbf{A}^{-1} = \\mathbf{I}\\] where \\(\\mathbf{I}\\) is an identify matrix (equivalent to 1). There is no division for matrices, instead we can use inverse to multiply the matrix by an inverse, similar to when instead of dividing the number \\(a\\) by \\(b\\) we multiply \\(a\\) by reciprocal of \\(b = \\frac{1}{b}\\) For a 2-dimensional matrix we can follow the below formula for obtaining the inverse \\[\\begin{bmatrix} x_{11} &amp; x_{12} \\\\ x_{21} &amp; x_{22} \\end{bmatrix}^{-1} = \\frac{1}{x_{11} \\cdot x_{22} - x_{12} \\cdot x_{21}} \\cdot \\begin{bmatrix} x_{22} &amp; -x_{12} \\\\ -x_{21} &amp; x_{11} \\end{bmatrix}\\] 7.5 Orthogonal matrix A matrix \\(\\mathbf{A}\\) for which \\(\\mathbf{A^\\top} = \\mathbf{A^{-1}}\\) is true is said to be orthogonal Exercise 7.1 Given matrices \\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}\\), \\(\\mathbf{B} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\) and \\(\\mathbf{C} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix}\\) what is the dimension of matrix \\(\\mathbf{A}\\)? what is \\(\\mathbf{A}^\\top\\)? which of the matrices is i) an identity matrix ii) a square matrix, iii) null matrix, iv) diagonal matrix, v) a triangular matrix,? calculate \\(\\mathbf{A} + \\mathbf{B}\\)? calculate \\(\\mathbf{A} \\cdot \\mathbf{C}\\)? calculate \\(\\mathbf{B}^\\top\\) calculate \\(\\mathbf{A}^{-1}\\) calculate \\((\\mathbf{A} + \\mathbf{B})^{-1}\\) Answers to selected exercises (matrices) Exr. 7.1 \\(2 \\times 2\\) \\(\\mathbf{A}^\\top = \\begin{bmatrix} 1 &amp; 3 \\\\ 2 &amp; 4 \\end{bmatrix}\\) identity matrix: \\(\\mathbf{B}\\), ii) a square matrix: \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\), iii) null matrix: none, iv) diagonal matrix: \\(\\mathbf{B}\\) (identity matrix is diagonal) and \\(\\mathbf{C}\\), v) triangular \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) as both identify matrix \\(\\mathbf{B}\\) and diagonal matrix \\(\\mathbf{C}\\) is triangular, both lower and upper triangular \\(\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix} + \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 2 \\\\ 3 &amp; 5 \\end{bmatrix}\\) \\(\\mathbf{A} \\cdot \\mathbf{C} = \\begin{bmatrix} 1 \\cdot 1 + 2 \\cdot 0 &amp; 1 \\cdot 0 + 2 \\cdot 2 \\\\ 3 \\cdot 1 + 4 \\cdot 0 &amp; 3 \\cdot 0 + 4 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 4 \\\\ 3 &amp; 8 \\end{bmatrix}\\) \\(\\mathbf{B}^\\top = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\) \\[\\mathbf{A}^{-1} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}^{-1} = \\frac{1}{1 \\cdot 4 - 2 \\cdot 3} \\cdot \\begin{bmatrix} 4 &amp; -2 \\\\ -3 &amp; 1 \\end{bmatrix} = -\\frac{1}{2} \\cdot \\begin{bmatrix} 4 &amp; -2 \\\\ -3 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} -2 &amp; 1 \\\\ \\frac{3}{2} &amp; -\\frac{1}{2} \\end{bmatrix}\\] "],
["introduction-to-linear-models.html", "Chapter 8 Introduction to linear models 8.1 Statistical vs. deterministic relationship 8.2 What linear models are and are not 8.3 Terminology 8.4 With linear models we can answer questions such as: 8.5 Simple linear regression 8.6 Least squares 8.7 Intercept and Slope 8.8 Hypothesis testing 8.9 Vector-matrix notations 8.10 Confidence intervals and prediction intervals 8.11 Exercises: linear models I Answers to selected exercises (linear models)", " Chapter 8 Introduction to linear models Aims to introduce concept of linear models using simple linear regression Learning outcomes to understand what a linear model is and be familiar with the terminology to be able to state linear model in the general vector-matrix notation to be able to use the general vector-matrix notation to numerically estimate model parameters to be able to use lm() function for model fitting, parameter estimation, hypothesis testing and prediction 8.1 Statistical vs. deterministic relationship Relationships in probability and statistics can generally be one of three things: deterministic, random, or statistical: a deterministic relationship involves an exact relationship between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation \\(Fahrenheit=\\frac{9}{5}\\cdot Celcius+32\\) there is no relationship between variables in the random relationship, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year a statistical relationship is a mixture of deterministic and random relationship, e.g. the savings that Olga has left in the bank account depend on Olga’s monthly salary income (deterministic part) and the money spent on buying succulents (random part) Figure 8.1: Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship ; b) statistical relationship between \\(x\\) and \\(y\\) is not perfect (increasing relationship), c) statistical relationship between \\(x\\) and \\(y\\) is not perfect (decreasing relationship), d) random signal 8.2 What linear models are and are not A linear model is one in which the parameters appear linearly in the deterministic part of the model e.g. simple linear regression through the origin is a simple linear model of the form \\(Y_i = \\beta x + \\epsilon\\) often used to express a relationship of one numerical variable to another, e.g. the calories burnt and the kilometers cycled linear models can become quite advanced by including more variables, e.g. the calories burnt could be a function of both the kilometers cycled and status of bike, or the transformation of the variables More examples where model parameters appear linearly: \\(Y_i = \\alpha + \\beta x_i + \\gamma x_i + \\epsilon_i\\) \\(Y_i = \\alpha + \\beta x_i^2 \\epsilon\\) \\(Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon\\) and an example on a non-linear model where parameter \\(\\beta\\) appears in the exponent of \\(x_i\\) \\(Y_i = \\alpha + x_i^\\beta + \\epsilon\\) 8.3 Terminology There are many terms and notations used interchangeably: \\(y\\) is being called: response outcome dependent variable \\(x\\) is being called: exposure explanatory variable dependent variable predictor covariate 8.4 With linear models we can answer questions such as: is there a relationship between exposure and outcome, e.g. body weight and plasma volume? how strong is the relationship between the two variables? what will be a predicted value of the outcome given a new set of exposure values? how accurately can we predict outcome? which variables are associated with the response, e.g. is it body weight and height that can explain the plasma volume or is it just the body weight? 8.5 Simple linear regression It is used to check the association between the numerical outcome and one numerical explanatory variable In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure For example, let’s look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is. Example data: weight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) plasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) Figure 8.2: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Figure 8.3: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable The equation for the red line is: \\[Y_i=0.086 + 0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8\\] and in general: \\[Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n\\] In other words, by finding the best-fitting straight line we are building a statistical model to represent the relationship between plasma volume (\\(Y\\)) and explanatory body weight variable (\\(x\\)) If were to use our model \\(Y_i=0.086 + 0.044 \\cdot x_i\\) to find plasma volume given a weight of 58 kg (our first observation, \\(i=1\\)), we would notice that we would get \\(Y=0.086 + 0.044 \\cdot 58 = 2.638\\), not exactly \\(2.75\\) as we have for our first man in our dataset that we started with, i.e. \\(2.75 - 2.638 = 0.112 \\neq 0\\). We thus add to the above equation an error term to account for this and now we can write our simple regression model more formally as: \\[\\begin{equation} Y_i=\\alpha + \\beta \\cdot x_i + \\epsilon_i \\tag{8.1} \\end{equation}\\] where: we call \\(\\alpha\\) and \\(\\beta\\) model coefficients and \\(\\epsilon_i\\) error terms 8.6 Least squares in the above body weight - plasma volume example, the values of \\(\\alpha\\) and \\(\\beta\\) have just appeared in practice, \\(\\alpha\\) and \\(\\beta\\) values are unknown and we use data to estimate these coefficients, noting the estimates with a hat, \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) least squares is one of the methods of parameters estimation, i.e. finding \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) Figure 8.4: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line Let \\(\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i\\) be the prediction \\(y_i\\) based on the \\(i\\)-th value of \\(x\\): Then \\(\\epsilon_i = y_i - \\hat{y_i}\\) represents the \\(i\\)-th residual, i.e. the difference between the \\(i\\)-th observed response value and the \\(i\\)-th response value that is predicted by the linear model RSS, the residual sum of squares is defined as: \\[RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2\\] or equivalently as: \\[RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2\\] the least squares approach chooses \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) to minimize the RSS. With some calculus we get Theorem 8.1 Theorem 8.1 (Least squares estimates for a simple linear regression) \\[\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}\\] \\[\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}\\] where: \\(\\bar{x}\\): mean value of \\(x\\) \\(\\bar{y}\\): mean value of \\(y\\) \\(S_{xx}\\): sum of squares of \\(X\\) defined as \\(S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2\\) \\(S_{yy}\\): sum of squares of \\(Y\\) defined as \\(S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2\\) \\(S_{xy}\\): sum of products of \\(X\\) and \\(Y\\) defined as \\(S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})\\) We can further re-write the above sum of squares to obtain sum of squares of \\(X\\), \\[S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n})\\] sum of products of \\(X\\) and \\(Y\\) \\[S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n}\\] Example (Least squares) Let’s try least squares method to find coefficient estimates in our body weight and plasma volume example # initial data weight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) plasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) # rename variables for convenience x &lt;- weight y &lt;- plasma # mean values of x and y x.bar &lt;- mean(x) y.bar &lt;- mean(y) # Sum of squares Sxx &lt;- sum((x - x.bar)^2) Sxy &lt;- sum((x-x.bar)*(y-y.bar)) # Coefficient estimates beta.hat &lt;- Sxy / Sxx alpha.hat &lt;- y.bar - Sxy/Sxx*x.bar # Print estimated coefficients alpha and beta print(alpha.hat) ## [1] 0.08572428 print(beta.hat) ## [1] 0.04361534 In R we can use lm, the built-in function, to fit a linear regression model and we can replace the above code with one line lm(plasma ~ weight) ## ## Call: ## lm(formula = plasma ~ weight) ## ## Coefficients: ## (Intercept) weight ## 0.08572 0.04362 8.7 Intercept and Slope Linear regression gives us estimates of model coefficient \\(Y_i = \\alpha + \\beta x_i + \\epsilon_i\\) \\(\\alpha\\) is known as the intercept \\(\\beta\\) is known as the slope Figure 8.5: Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red) 8.8 Hypothesis testing the calculated \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are estimates of the population values of the intercept and slope and are therefore subject to sampling variation their precision is measure by their ** estimated standard errors**, e.s.e(\\(\\hat{\\alpha}\\)) and e.s.e(\\(\\hat{\\beta}\\)) these estimated standard errors are used in hypothesis testing and building confidence and prediction intervals The most common hypothesis test involves testing the null hypothesis of: \\(H_0:\\) There is no relationship between \\(X\\) and \\(Y\\) versus the alternative hypothesis \\(H_a:\\) there is some relationship between \\(X\\) and \\(Y\\) Mathematically, this corresponds to testing: \\(H_0: \\beta=0\\) versus \\(H_0: \\beta\\neq0\\) since if \\(\\beta=0\\) then the model \\(Y_i=\\alpha+\\beta x_i + \\epsilon_i\\) reduces to \\(Y=\\alpha + \\epsilon_i\\) Under the null hypothesis: \\(H_0: \\beta = 0\\) we have: \\(\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p)\\), where \\(n\\) is number of observations \\(p\\) is number of model parameters \\(\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}\\) is called the t-statistics that follows Student’s t distribution with \\(n-p\\) degrees of freedom Example (Hypothesis testing) Let’s look again at our example data. This time we will not only fit the linear regression model but look a bit more closely at the R summary of the model weight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) plasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) model &lt;- lm(plasma ~ weight) print(summary(model)) ## ## Call: ## lm(formula = plasma ~ weight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.27880 -0.14178 -0.01928 0.13986 0.32939 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.08572 1.02400 0.084 0.9360 ## weight 0.04362 0.01527 2.857 0.0289 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2188 on 6 degrees of freedom ## Multiple R-squared: 0.5763,\tAdjusted R-squared: 0.5057 ## F-statistic: 8.16 on 1 and 6 DF, p-value: 0.02893 Under “Estimate” we see estimates of our model coefficients, \\(\\hat{\\alpha}\\) (intercept) and \\(\\hat{\\beta}\\) (slope, here weight), followed by their estimated standard errors. If we were to test if there is an association between weight and plasma volume we would write under \\(H_0: \\beta = 0\\) and \\(\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582\\) and we would compare t-statistics to Student’s t distribution with \\(n-p = 8 - 2 = 6\\) degrees of freedom (we have two model parameters, \\(\\alpha\\) and \\(\\beta\\)) we can use Student’s t distribution table or R code to obtain the p-value 2*pt(2.856582, df=6, lower=F) ## [1] 0.02893095 here the observed t-statistics is large and therefore yields a small p-value, meaning that there is sufficient evidence to reject null hypothesis in favor of the alternative and conclude that there is an significant association between weight and plasma volume 8.9 Vector-matrix notations While in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings with multiple regression (more than one explanatory variable in the model) it is more efficient to use vectors and matrices to define the regression model. Let’s rewrite our simple linear regression model \\(Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n\\) into vector-matrix notations. First we rename our \\(\\alpha\\) to \\(\\beta_0\\) and \\(\\beta\\) to \\(\\beta_1\\) (it is easier to keep tracking the number of model parameters this way) Then we notice that we actually have \\(n\\) equations such as: \\[y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1\\] \\[y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2\\] \\[y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3\\] \\[\\dots\\] \\[y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n\\] we can group all \\(Y_i\\) and \\(\\epsilon_i\\) into column vectors: \\(\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}\\) and \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}\\) we stack two parameters \\(\\beta_0\\) and \\(\\beta_1\\) into another column vector:\\[\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\] we then append a vector of ones with the single predictor for each \\(i\\) and create a matrix with two columns: design matrix \\[\\mathbf{X}=\\begin{bmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n} \\end{bmatrix}\\] Now we can write our linear model in a vector-matrix notations as: \\[\\mathbf{Y} = \\boldsymbol\\beta\\mathbf{X} + \\boldsymbol\\epsilon\\] Definition: vector matrix form of the linear model The vector-matrix representation of a linear model with \\(p-1\\) predictors can be written as \\[\\mathbf{Y} = \\boldsymbol\\beta\\mathbf{X} + \\boldsymbol\\epsilon\\] where: \\(\\mathbf{Y}\\) is \\(n \\times1\\) vector of observations \\(\\boldsymbol\\beta\\) is \\(p \\times1\\) vector of parameters \\(\\mathbf{X}\\) is \\(n \\times p\\) design matrix \\(\\boldsymbol\\epsilon\\) is \\(n \\times1\\) vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, \\(\\sigma^2\\)) In full, the above vectors and matrix have the form: \\(\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}\\) \\(\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix}\\) \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}\\) \\(\\mathbf{X}=\\begin{bmatrix} 1 &amp; x_{1,1} &amp; \\dots &amp; x_{1,p-1} \\\\ 1 &amp; x_{2,1} &amp; \\dots &amp; x_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n,1} &amp; \\dots &amp; x_{n,p-1} \\end{bmatrix}\\) Theorem 8.2 (Least squares in vector-matrix notation) The least squares estimates for a linear regression of the form: \\[\\mathbf{Y} = \\boldsymbol\\beta\\mathbf{X} + \\boldsymbol\\epsilon\\] is given by: \\[\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\] Example: vector-matrix notation Following the above definition we can write our weight - plasma volume model as: \\[\\mathbf{Y} = \\boldsymbol\\beta\\mathbf{X} + \\boldsymbol\\epsilon\\] where: \\(\\mathbf{Y}=\\begin{bmatrix} 2.75 \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12 \\end{bmatrix}\\) \\(\\boldsymbol\\beta=\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\) \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{8} \\end{bmatrix}\\) \\(\\mathbf{X}=\\begin{bmatrix} 1 &amp; 58.0 \\\\ 1 &amp; 70.0 \\\\ 1 &amp; 74.0 \\\\ 1 &amp; 63.5 \\\\ 1 &amp; 62.0 \\\\ 1 &amp; 70.5 \\\\ 1 &amp; 71.0 \\\\ 1 &amp; 66.0 \\\\ \\end{bmatrix}\\) and we can estimate model parameters using \\(\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\). We can do it by hand or in R as follows: n &lt;- length(plasma) # no. of observation Y &lt;- as.matrix(plasma, ncol=1) X &lt;- cbind(rep(1, length=n), weight) X &lt;- as.matrix(X) # print Y and X to double-check that the format is according to the definition print(Y) ## [,1] ## [1,] 2.75 ## [2,] 2.86 ## [3,] 3.37 ## [4,] 2.76 ## [5,] 2.62 ## [6,] 3.49 ## [7,] 3.05 ## [8,] 3.12 print(X) ## weight ## [1,] 1 58.0 ## [2,] 1 70.0 ## [3,] 1 74.0 ## [4,] 1 63.5 ## [5,] 1 62.0 ## [6,] 1 70.5 ## [7,] 1 71.0 ## [8,] 1 66.0 # least squares estimate # solve() finds inverse of matrix beta.hat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y print(beta.hat) ## [,1] ## 0.08572428 ## weight 0.04361534 8.10 Confidence intervals and prediction intervals when we estimate coefficients we can also find their confidence intervals, typically 95% confidence intervals, i.e. a range of vales that contain the true unknown value of the parameter we can also use linear regression models to predict the response value given a new observation and find prediction intervals. Here, we look at any specific value of \\(x_i\\), and find an interval around the predicted value \\(y_i&#39;\\) for \\(x_i\\) such that there is a 95% probability that the real value of y (in the population) corresponding to \\(x_i\\) is within this interval Earlier we said that we use estimated standard error in hypothesis testing and in finding the intervals but we have not yet said how to calculate e.s.e. Using vector-matrix notation we can now write that: \\[\\frac{(\\mathbf{b}\\hat{{\\boldsymbol\\beta}}-\\mathbf{b}^T\\boldsymbol\\beta)}{\\sqrt{\\frac{RSS}{n-p}\\mathbf{b^T(X^TX)^{-1}b}}}\\] where: the denominator would yield e.s.e(\\(\\beta_1\\)) if \\(\\mathbf{b^T}=(0 \\quad 1)\\) and a model \\(Y_i = \\beta_0 + \\beta_1x + \\epsilon_i\\) a confidence interval estimate for \\(\\beta_1\\) could be estimated via: \\[\\mathbf{b^T}\\hat{\\boldsymbol\\beta} \\pm (n-p; \\frac{1+c}{2})\\sqrt{\\frac{RSS}{n-p}(\\mathbf{b^T}(\\mathbf{X^T}\\mathbf{X})^{-1}\\mathbf{b}))}\\] and a prediction interval with confidence \\(c\\) is \\[\\mathbf{b^T}\\hat{\\boldsymbol\\beta} \\pm (n-p; \\frac{1+c}{2})\\sqrt{(\\frac{RSS}{n-p}(1+\\mathbf{b^T}(\\mathbf{X^T}\\mathbf{X})^{-1}\\mathbf{b}})\\] We will not go further into these calculations here but use R functions to obtain these just remember that the prediction interval is always wider than the confidence interval note (1 + ) in the prediction interval equation Example: prediction and intervals Let’s: find confidence intervals for our coefficient estimates predict plasma volume for a men weighting 60 kg find prediction interval plot original data, fitted regression model, predicted observation # fit regression model model &lt;- lm(plasma ~ weight) print(summary(model)) ## ## Call: ## lm(formula = plasma ~ weight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.27880 -0.14178 -0.01928 0.13986 0.32939 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.08572 1.02400 0.084 0.9360 ## weight 0.04362 0.01527 2.857 0.0289 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2188 on 6 degrees of freedom ## Multiple R-squared: 0.5763,\tAdjusted R-squared: 0.5057 ## F-statistic: 8.16 on 1 and 6 DF, p-value: 0.02893 # find confidence intervals for the model coefficients confint(model) ## 2.5 % 97.5 % ## (Intercept) -2.419908594 2.59135716 ## weight 0.006255005 0.08097567 # predict plasma volume for a new observation of 60 kg # we have to create data frame with a variable name matching the one used to build the model new.obs &lt;- data.frame(weight = 60) predict(model, newdata = new.obs) ## 1 ## 2.702645 # find prediction intervals predict(model, newdata = new.obs, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 2.702645 2.079373 3.325916 # plot the original data, fitted regression and predicted value plot(weight, plasma, pch=19, xlab=&quot;weight [kg]&quot;, ylab=&quot;plasma [l]&quot;) lines(weight, model$fitted.values, col=&quot;red&quot;) # fitted model in red points(new.obs, predict(model, newdata = new.obs), pch=19, col=&quot;blue&quot;) # predicted value at 60kg 8.11 Exercises: linear models I Exercise 8.1 Linear models form Which of the following models are linear models and why? \\(Y_i=\\alpha + \\beta x_i + \\epsilon_i\\) \\(Y_i=\\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\epsilon_i\\) \\(Y_i=\\alpha + \\beta x_i + \\gamma x_i^2 + \\epsilon_i\\) \\(Y_i=\\alpha + \\gamma x_i^\\beta + \\epsilon_i\\) Exercise 8.2 Protein levels in pregnancy The researchers were interested whether protein levels in expectant mothers are changing throughout the pregnancy. Observations have been taken on 19 healthy women and each woman was at different stage of pregnancy (gestation). Assuming linear model: \\(Y_i = \\alpha + \\beta x_i + \\epsilon_i\\), where \\(Y_i\\) corresponds to protein levels in i-th observation and taking summary statisitcs: \\(\\sum_{i=1}^{n}x_i = 456\\) \\(\\sum_{i=1}^{n}x_i^2 = 12164\\) \\(\\sum_{i=1}^{n}x_iy_i = 369.87\\) \\(\\sum_{i=1}^{n}y_i = 14.25\\) \\(\\sum_{i=1}^{n}y_i^2 = 11.55\\) find the least square estimates of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) knowing that e.s.e(\\(\\hat{\\beta}) = 0.022844\\) can we: reject the null hypothesis that the is no relationship between protein level and gestation, i.e. perform a hypothesis test to test \\(H_0:\\beta = 0\\); can we reject the null hypothesis that \\(\\beta = 0.02\\), i.e. perform a hypothesis test to test \\(H_0:\\beta = 0.02\\) write down the linear model in the vector-matrix notation and identify response, parameter, design and error matrices read in “protein.csv” data into R, set Y as protein (response) and calculate using matrix functions the least squares estimates of model coefficients use lm() function in R to check your calculations use the fitted model in R to predict the value of protein levels at week 20. Try plotting the data, fitted linear model and the predicted value to assess whether your prediction is to be expected. Exercise 8.3 The glucose level in potatoes depends on their storage time and the relationship is somehow curvilinear as shown below. As we believe that the quadratic function might describe the relationship, assume linear model in form \\(Y_i = \\alpha + \\beta x_i + \\gamma x_i^2 + \\epsilon_i \\quad i=1,\\dots,n\\) where \\(n=14\\) and write down the model in vector-matrix notation load data to from “potatoes.csv” and use least squares estimates for obtain estimates of model coefficients perform a hypothesis test to test \\(H_0:\\gamma=0\\); and comment whether there is a significant quadratic relationship use lm() function to verify your calculations predict glucose concentration at storage time 4 and 16 weeks. Plot the data, the fitted model and the predicted values Figure 8.6: Sugar in potatoes: relationship between storage time and glucose content Answers to selected exercises (linear models) Exr. 8.2 \\(S_{xx} = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n} = 12164 - \\frac{456^2}{19} = 1220\\) \\(S_{xy} = \\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n} = 369.87 - \\frac{(456 \\cdot 14.25)}{19} = 27.87\\) \\(\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}} = 27.87 / 1220 = 0.02284\\) \\(\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x} = \\frac{14.25}{19}-\\frac{27.87}{1220}\\cdot \\frac{456}{19} = 0.20174\\) We can calculate test statistics following: \\(\\frac{\\hat{\\beta} - \\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p) = \\frac{0.02284 - 0}{0.20174} = 6.934\\) where the value follows Student’s t distribution with \\(n-p = 19 - 2 = 17\\) degrees of freedom. We can now estimate the a p-value using Student’s t distribution table or use R function 2*pt(6.934, df=17, lower=F) ## [1] 2.414315e-06 As p-value &lt;&lt; 0.001 there is sufficient evidence to reject \\(H_0\\) in favor of \\(H_1\\), thus we can conclude that there is a significant relationship between protein levels and gestation Similarly, we can test \\(H_0:\\beta = 0.02\\), i.e. \\(\\frac{\\hat{\\beta} - \\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p) = \\frac{0.02284 - 0.02}{0.20174} = 0.01407753\\). Now the test statistics is small 2*pt(0.01407753, df=17, lower=F) ## [1] 0.988932 p-value is large and hence there is no sufficient evidence to reject \\(H_0\\) and we can conclude that \\(\\beta = 0.02\\) We can rewrite the linear model in vector-matrix formation as \\(\\mathbf{Y}= \\mathbf{\\beta}\\mathbf{X} + \\mathbf{\\epsilon}\\) where: response \\(\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{19} \\end{bmatrix}\\) parameters \\(\\boldsymbol\\beta=\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}\\) design matrix \\(\\mathbf{X}=\\begin{bmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{19} \\end{bmatrix}\\) errors \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{19} \\end{bmatrix}\\) The least squares estimates in vector-matrix notation is \\(\\hat{\\boldsymbol\\beta}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and we can calculate this in R # read in data data.protein &lt;- read.csv(&quot;data/lm/protein.csv&quot;) # print out top observations head(data.protein) ## Protein Gestation ## 1 0.38 11 ## 2 0.58 12 ## 3 0.51 13 ## 4 0.38 15 ## 5 0.58 17 ## 6 0.67 18 # define Y and X matrices given the data n &lt;- nrow(data.protein) # nu. of observations Y &lt;- as.matrix(data.protein$Protein, ncol=1) # response X &lt;- as.matrix(cbind(rep(1, length=n), data.protein$Gestation)) # design matrix head(X) # double check that the design matrix looks like it should ## [,1] [,2] ## [1,] 1 11 ## [2,] 1 12 ## [3,] 1 13 ## [4,] 1 15 ## [5,] 1 17 ## [6,] 1 18 # least squares estimate beta.hat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y # beta.hat is a matrix that contains our alpha and beta in the model print(beta.hat) ## [,1] ## [1,] 0.20173770 ## [2,] 0.02284426 We use lm() function to check our calculations # fit linear regression model and print model summary protein &lt;- data.protein$Protein # our Y gestation &lt;- data.protein$Gestation # our X model &lt;- lm(protein ~ gestation) print(summary(model)) ## ## Call: ## lm(formula = protein ~ gestation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.16853 -0.08720 -0.01009 0.08578 0.20422 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.201738 0.083363 2.420 0.027 * ## gestation 0.022844 0.003295 6.934 2.42e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1151 on 17 degrees of freedom ## Multiple R-squared: 0.7388,\tAdjusted R-squared: 0.7234 ## F-statistic: 48.08 on 1 and 17 DF, p-value: 2.416e-06 new.obs &lt;- data.frame(gestation = 20) y.pred &lt;- predict(model, newdata = new.obs) # we can visualize the data, fitted linear model (red), and the predicted value (blue) plot(gestation, protein, pch=19, xlab=&quot;gestation [weeks]&quot;, ylab=&quot;protein levels [mgml-1]&quot;) lines(gestation, model$fitted.values, col=&quot;red&quot;) points(new.obs, y.pred, col=&quot;blue&quot;, pch=19, cex = 1) Exr. 8.3 We can rewrite the linear model in vector-matrix formation as \\(\\mathbf{Y}= \\boldsymbol\\beta\\mathbf{X} + \\mathbf{\\epsilon}\\) where: response \\(\\mathbf{Y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{14} \\end{bmatrix}\\) parameters \\(\\boldsymbol\\beta=\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{bmatrix}\\) design matrix \\(\\mathbf{X}=\\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2\\\\ 1 &amp; x_2 &amp; x_2^2\\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{14} &amp; x_{14}^2 \\end{bmatrix}\\) errors \\(\\boldsymbol\\epsilon=\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{14} \\end{bmatrix}\\) load data to from “potatoes.csv” and use least squares estimates for obtain estimates of model coefficients data.potatoes &lt;- read.csv(&quot;data/lm/potatoes.csv&quot;) # define matrices n &lt;- nrow(data.potatoes) Y &lt;- data.potatoes$Glucose X1 &lt;- data.potatoes$Weeks X2 &lt;- (data.potatoes$Weeks)^2 X &lt;- cbind(rep(1, length(n)), X1, X2) X &lt;- as.matrix(X) # least squares estimate # beta here refers to the matrix of model coefficients incl. alpha, beta and gamma beta.hat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y print(beta.hat) ## [,1] ## 200.169312 ## X1 -19.443122 ## X2 1.030423 we use lm() function to verify our calculations: model &lt;- lm(Y ~ X1 + X2) print(summary(model)) ## ## Call: ## lm(formula = Y ~ X1 + X2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.405 -11.250 -8.071 12.911 29.286 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 200.1693 15.0527 13.298 4.02e-08 *** ## X1 -19.4431 3.1780 -6.118 7.54e-05 *** ## X2 1.0304 0.1406 7.329 1.49e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.4 on 11 degrees of freedom ## Multiple R-squared: 0.8694,\tAdjusted R-squared: 0.8457 ## F-statistic: 36.61 on 2 and 11 DF, p-value: 1.373e-05 perform a hypothesis test to test \\(H_0:\\gamma=0\\); and comment whether we there is a significant quadratic term \\(\\frac{\\hat{\\gamma} - \\gamma}{e.s.e(\\hat{\\gamma})} \\sim t(n-p) = \\frac{1.030423 - 0}{0.1406} = 7.328755\\) where the value follows Student’s t distribution with \\(n-p = 19 - 2 = 17\\) degrees of freedom. We can now estimate the a p-value using Student’s t distribution table or use a function in R 2*pt(7.328755, df=14-3, lower=F) ## [1] 1.487682e-05 As p-value &lt;&lt; 0.001 there is sufficient evidence to reject \\(H_0\\) in favor of \\(H_1\\), thus we can conclude that there is a significant quadratic relationship between glucose and storage time predict glucose concentration at storage time 4 and 16 weeks new.obs &lt;- data.frame(X1 = c(4, 16), X2 = c(4^2, 16^2)) pred.y &lt;- predict(model, newdata = new.obs) plot(data.potatoes$Weeks, data.potatoes$Glucose, xlab=&quot;Storage time [weeks]&quot;, ylab=&quot;Glucose [g/kg]&quot;, pch=19) lines(data.potatoes$Weeks, model$fitted.values, col=&quot;red&quot;) points(new.obs[,1], pred.y, pch=19, col=&quot;blue&quot;) "],
["regression-coefficients.html", "Chapter 9 Regression coefficients 9.1 Interpreting and using linear regression models 9.2 Example: plasma volume 9.3 Example: Galapagos Islands 9.4 Example: Height and gender 9.5 Example: Heigth, weight and gender I 9.6 Example: Heigth, weight and gender II 9.7 Exercises: linear models II Answers to selected exercises (linear models II)", " Chapter 9 Regression coefficients Aims to clarify the interpretation of the fitted linear models Learning outcomes to use lm() function to fit multiple linear regression model to be able to interpret the output of the model to be able to use lm() function to check for association between variables, group effects and interaction terms 9.1 Interpreting and using linear regression models In previous section we have seen how to find estimates of model coefficients, using theorems and vector-matrix notations. Now, we will focus on what model coefficient values tell us and how to interpret them And we will look at the common cases of using linear regression models We will do this via analyzing some examples 9.2 Example: plasma volume # data weight &lt;- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) plasma &lt;- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) # fit regression model model &lt;- lm(plasma ~ weight) # plot the original data and fitted regression line plot(weight, plasma, pch=19, xlab=&quot;weight [kg]&quot;, ylab=&quot;plasma [l]&quot;) lines(weight, model$fitted.values, col=&quot;red&quot;) # fitted model in red grid() # print model summary print(summary(model)) ## ## Call: ## lm(formula = plasma ~ weight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.27880 -0.14178 -0.01928 0.13986 0.32939 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.08572 1.02400 0.084 0.9360 ## weight 0.04362 0.01527 2.857 0.0289 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2188 on 6 degrees of freedom ## Multiple R-squared: 0.5763,\tAdjusted R-squared: 0.5057 ## F-statistic: 8.16 on 1 and 6 DF, p-value: 0.02893 Model: \\(Y_i = \\alpha + \\beta x_i + \\epsilon_i\\) where \\(x_i\\) corresponds to \\(weight_i\\) Slope The value of slope tells us how and by much the outcome changes with a unit change in \\(x\\) If we go up in weight 1 kg what would be our expected change in plasma volume? _ Answer: it would increase by 0.04 liter And if we go up in weight 10 kg what would be our expected change in plasma volume? Answer: it would increase by \\(0.04 \\cdot 10 = 0.4\\) liter Intercept the intercept, often labeled the constant, is the value of Y when \\(x_i=0\\) in models where \\(x_i\\) can be equal 0, the intercept is simply the expected mean value of response in models where \\(x_i\\) cannot be equal 0, like in our plasma example no weight makes no sense for healthy men, the intercept has no intrinsic meaning the intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome 9.3 Example: Galapagos Islands Researchers were interested in biological diversity on the Galapagos islands. They’ve collected data on number of plant species (Species) and number of endemic species on 30 islands as well as some descriptors of the islands such as area [\\(\\mathrm{km^2}\\)], elevation [m], distance to nearest island [km], distance to Santa Cruz [km] and the area of the adjacent island [\\(\\mathrm{km^2}\\)]. The preview of data is here: # Data is available via faraway package if(!require(faraway)){ install.packages(&quot;faraway&quot;) library(faraway) } head(gala) ## Species Endemics Area Elevation Nearest Scruz Adjacent ## Baltra 58 23 25.09 346 0.6 0.6 1.84 ## Bartolome 31 21 1.24 109 0.6 26.3 572.33 ## Caldwell 3 3 0.21 114 2.8 58.7 0.78 ## Champion 25 9 0.10 46 1.9 47.4 0.18 ## Coamano 2 1 0.05 77 1.9 1.9 903.82 ## Daphne.Major 18 11 0.34 119 8.0 8.0 1.84 And we can fit a linear regression model to model number of Species given the remaining variables. Let’s keep aside for now that number of Species is actually a count variable, not a continuous numerical variable, we just want to estimate the number of Species for now. # fit multiple linear regression and print model summary model1 &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala) print(summary(model1)) ## ## Call: ## lm(formula = Species ~ Area + Elevation + Nearest + Scruz + Adjacent, ## data = gala) ## ## Residuals: ## Min 1Q Median 3Q Max ## -111.679 -34.898 -7.862 33.460 182.584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.068221 19.154198 0.369 0.715351 ## Area -0.023938 0.022422 -1.068 0.296318 ## Elevation 0.319465 0.053663 5.953 3.82e-06 *** ## Nearest 0.009144 1.054136 0.009 0.993151 ## Scruz -0.240524 0.215402 -1.117 0.275208 ## Adjacent -0.074805 0.017700 -4.226 0.000297 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60.98 on 24 degrees of freedom ## Multiple R-squared: 0.7658,\tAdjusted R-squared: 0.7171 ## F-statistic: 15.7 on 5 and 24 DF, p-value: 6.838e-07 Model \\(Y_i = \\beta_0 + \\beta_1 Area_i + \\beta_2 Area_i + \\beta_3 Elevation_i + \\beta_4 Scruz_i + \\beta_5 Adjacent_i + \\epsilon_i\\) Slope Compare two islands, where the second island has an elevation one meter higher than the first one, what can we say about the number of species according to the model? Answer: the second island will have 0.32 species more than the first one How about if the difference is 100 m? Answer: now the second island will have \\(0.32 \\cdot 100 = 32\\) more species Not so easy Consider an alternative model where we only use evaluation to model the number of species model2 &lt;- lm(Species ~ Elevation, data = gala) print(summary(model2)) ## ## Call: ## lm(formula = Species ~ Elevation, data = gala) ## ## Residuals: ## Min 1Q Median 3Q Max ## -218.319 -30.721 -14.690 4.634 259.180 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.33511 19.20529 0.590 0.56 ## Elevation 0.20079 0.03465 5.795 3.18e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 78.66 on 28 degrees of freedom ## Multiple R-squared: 0.5454,\tAdjusted R-squared: 0.5291 ## F-statistic: 33.59 on 1 and 28 DF, p-value: 3.177e-06 Model \\(Y_i = \\beta_0 + \\beta_1 Elevation_i + \\epsilon_i\\) Slope Compare two islands, where the second island has an elevation one meter higher than the first one, what can we say about the number of species according to the model? Answer: the second island will have 0.20 species more than the first one How about if the difference is 100 m? Answer: now the second island will have \\(0.20 \\cdot 100 = 20\\) more species Specific interpretation obviously there is difference between 32 and 20 species given the same elevation difference and using the two different models our interpretations need to be more specific and we say a unit increase in \\(x\\) with other predictors held constant will produce a change equal to \\(\\hat{\\beta}\\) in the response \\(y\\) it is of course often quite unrealistic to be able to control other variables and keep them constant and for our simple regression, model 2, a change in evaluation is most likely associated with other variables, even though they are not included in the model further, our explanation contains no notation of causation, even thought the two models are showing a strong association between elevation and number of species we will learn later how to assess models and select variables (feature selection), here, we continue focusing on learning how to interpret the coefficients given a model 9.4 Example: Height and gender Data are available containing the weight [lbs] and height [inches] of 10000 men and women We want to compare the average height of men and women We can do that using linear regression and including gender as binary variable # read in data htwtgen &lt;- read.csv(&quot;data/lm/heights_weights_genders.csv&quot;) head(htwtgen) ## Gender Height Weight ## 1 Male 73.84702 241.8936 ## 2 Male 68.78190 162.3105 ## 3 Male 74.11011 212.7409 ## 4 Male 71.73098 220.0425 ## 5 Male 69.88180 206.3498 ## 6 Male 67.25302 152.2122 # boxplot for females and males boxplot(htwtgen$Height ~ htwtgen$Gender, xlab=&quot;&quot;, ylab=&quot;Height&quot;, col=&quot;lightblue&quot;) Model \\[Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i\\] where \\[\\begin{equation} I_{x_i} = \\left\\{ \\begin{array}{cc} 1 &amp; \\mathrm{if\\ } x_i=1 \\\\ 0 &amp; \\mathrm{if\\ } x_i=0 \\\\ \\end{array} \\right. \\end{equation}\\] for some coding, e.g. we choose to set “Female=1” and “Male=0” or vice versa. # fit linear regression and print model summary model1 &lt;- lm(Height ~ Gender, data = htwtgen) print(summary(model1)) ## ## Call: ## lm(formula = Height ~ Gender, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.6194 -1.8374 0.0088 1.9185 9.9724 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 63.70877 0.03933 1619.8 &lt;2e-16 *** ## GenderMale 5.31757 0.05562 95.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.781 on 9998 degrees of freedom ## Multiple R-squared: 0.4776,\tAdjusted R-squared: 0.4775 ## F-statistic: 9140 on 1 and 9998 DF, p-value: &lt; 2.2e-16 Estimates \\[\\hat{\\alpha} = 63.71\\] \\[\\hat{\\beta} = 5.32\\] The lm() function choose one of the category as baseline, here Females model summary prints the output of the model with the baseline category “hidden” notice the only label we have is “GenderMale” meaning that we ended-up having a model coded as below: \\[\\begin{equation} I_{x_i} = \\left\\{ \\begin{array}{cc} 1 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\ 0 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\ \\end{array} \\right. \\end{equation}\\] Consequently, if observation \\(i\\) is male then the expected value of height is: \\[E(Height_i|Male) = 63.71 + 5.32 = 69.03\\] and if observation \\(i\\) is female then the expected value of height is: \\[E(Height_i|Male) = 63.71\\] 9.5 Example: Heigth, weight and gender I So there is a difference in height between the gender, as expected Is there a relationship between weight and height? If so, does this relationship depend on gender? library(ggplot2) # plot the data separately for Male and Female ggplot(data=htwtgen, aes(x = Weight, y=Height, col = Gender)) + geom_point(alpha = 0.5) From the plot we can see that height increases with weight Males have higher heights than females Males have higher weights than females The relationship between height and weight appears to be the same for males and females, i.e. height increass with weight for both men and women Model \\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation} I_{x_i} = \\left\\{ \\begin{array}{cc} 1 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\ 0 &amp; \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\ \\end{array} \\right. \\end{equation}\\] and \\(x_{2,i}\\) is the weight of person \\(i\\) # Fit linear model and print model summary model2 &lt;- lm(Height ~ Gender + Weight, data = htwtgen) print(summary(model2)) ## ## Call: ## lm(formula = Height ~ Gender + Weight, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.4956 -0.9583 0.0126 0.9867 5.8358 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.0306678 0.1025161 458.76 &lt;2e-16 *** ## GenderMale -0.9628643 0.0474947 -20.27 &lt;2e-16 *** ## Weight 0.1227594 0.0007396 165.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.435 on 9997 degrees of freedom ## Multiple R-squared: 0.8609,\tAdjusted R-squared: 0.8609 ## F-statistic: 3.093e+04 on 2 and 9997 DF, p-value: &lt; 2.2e-16 Estimates \\[\\hat{\\alpha} = 47.031\\] \\[\\hat{\\beta} = -0.963\\] \\[\\hat{\\gamma} = 0.123\\] therefore, for a male of weight 161.4 we would predict a height of: \\[E(Height_i|Male, Weight = 161.4) = 47.031 - 0.963 + (0.123 \\cdot 161.4) = 65.9\\] and for a female of weight 161.4 we would predict a height of \\[E(Height_i|Female, Weight = 161.4) = 47.031 + (0.123 \\cdot 161.4) = 66.9\\] as much as our calculations are correct, the above example also shows the need of considering the data when interpreting coefficients. Our expected female height at weight 161.4 is 66.9, larger than the men height, 65.9. However, our data show that the the majority of our values for female lie between 100 and 175 and between 150 and 250 for males, i.e. in different ranges we should not predict outside the data range finally, we can plot our data and the fitted model # plot the data separately for Male and Female # using ggplot() and geom_smooth() ggplot(data=htwtgen, aes(x = Weight, y=Height, col = Gender)) + geom_point(alpha = 0.1) + geom_smooth(method=lm) 9.6 Example: Heigth, weight and gender II The fitted lines in the above example are parallel, the slope is modeled to be the same for male and females, and the intercept denotes the group differences It is also possible to allow both intercept and slope being fitted separately for each group This is done when we except that the relationships are different in different groups And we then talk about including interaction effect Model \\[Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}\\] where: \\(Y_{i,j}\\) is the height of person \\(j\\) of gender \\(i\\) \\(x_{ij}\\) is the weight of person \\(j\\) of gender \\(i\\) \\(i=1\\) corresponds to males in our example (keeping the same coding as above) \\(i=2\\) corresponds to females # fit linear model with interaction model3 &lt;- lm(Height ~ Gender * Weight, data = htwtgen) print(summary(model3)) ## ## Call: ## lm(formula = Height ~ Gender * Weight, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.4698 -0.9568 0.0092 0.9818 5.7544 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.347783 0.146325 323.579 &lt; 2e-16 *** ## GenderMale -1.683668 0.242119 -6.954 3.78e-12 *** ## Weight 0.120425 0.001067 112.903 &lt; 2e-16 *** ## GenderMale:Weight 0.004493 0.001480 3.036 0.0024 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.435 on 9996 degrees of freedom ## Multiple R-squared: 0.861,\tAdjusted R-squared: 0.861 ## F-statistic: 2.064e+04 on 3 and 9996 DF, p-value: &lt; 2.2e-16 Now, based on the regression output we would expect: for a male of weight \\(x\\), a height of: \\[E(height|male\\; and \\; weight=x)=47.34778 - 1.68367 + 0.12043x + 0.00449x = 45.7 + 0.125x\\] for a female of weight \\(x\\), a height of \\[E(height|female\\; and \\; weight=x)=47.34778 + 0.12043x\\] Estimates \\[\\hat{\\alpha_1} = 45.7\\] \\[\\hat{\\beta_1} = 0.125\\] \\[\\hat{\\alpha_2} = 47.34778\\] \\[\\hat{\\beta_2} = 0.12043\\] we can see from the regression output that the interaction term, \"GenderMale:Weight, is significant and therefore the relationship between weight and height is different in males and females we can plot the fitted model now and see that the lines are no longer parallel we will see clearer example of interactions in exercises # ggiraphExtra makes it easy to visualize fitted models if(!require(ggiraphExtra)){ install.packages(&quot;ggiraphExtra&quot;) library(ggiraphExtra) } ggPredict(model3) 9.7 Exercises: linear models II Exercise 9.1 Given the “height-weight-gender” data: repeat fitting the models with a) gender, b) weight and gender and c) interaction between weight and gender given the model with the interaction term, what is expected height of males and females give weight of 120 lbs? can you use predict() function to check your calculations? Exercise 9.2 When the behavior of a group of trout is studied, some fish are observed to become dominant and others to become subordinate. Dominant fish have freedom of movement whereas subordinate fish tend to congregate in the periphery of the waterway to avoid crossing the path of the dominant fish. Data on energy expenditure and ration of blood obtained were collected as part of a laboratory experiment for 20 trout. Energy and ration is measured in calories per kilo-calorie per trout per day. Use the below code to load the data to R and use linear regression models to answer: is there a relationship between ration obtained and energy expenditure is the relationship between ration obtained and energy expenditure different for each type of fish? Hint: it is good to start with some explanatory plots between every pair of variable # read in data and show preview trout &lt;- read.csv(&quot;data/lm/trout.csv&quot;) # recode the Group variable and treat like categories (factor) trout$Group &lt;- factor(trout$Group, labels=c(&quot;Dominant&quot;, &quot;Subordinate&quot;)) Exercise 9.3 A clinical trial A clinical trial has been carried out to compare three drug treatments which are intended to lower blood pressure in hypertensive patients. The data contains initial values fo systolic blood pressure (bp) in mmHg for each patient and the reduction achieved during the course of the trial. For each patient, allocation to treatment (drug) was carried out randomly and conditions such as the length of the treatment and dose of the drug were standardized as far as possible. Use linear regression to answer questions: is there an association between the reduction in blood pressure and initial blood pressure is reduction in blood pressure different across the treatment (in three drug groups)? is reduction in blood pressure different across the treatment when accounting for initial blood pressure? is reduction in blood pressure chaining differently under different treatment? Hint: here we have three categories which can be seen as expalnding the models with tow categories by an additional one: one category will be treated as baseline blooddrug &lt;- read.csv(&quot;data/lm/bloodrug.csv&quot;) blooddrug$drug &lt;- factor(blooddrug$drug) head(blooddrug) ## initial redn drug ## 1 158 4 1 ## 2 176 21 1 ## 3 174 36 1 ## 4 168 14 1 ## 5 174 34 1 ## 6 186 37 1 Answers to selected exercises (linear models II) Exr. 9.1 htwtgen &lt;- read.csv(&quot;data/lm/heights_weights_genders.csv&quot;) head(htwtgen) ## Gender Height Weight ## 1 Male 73.84702 241.8936 ## 2 Male 68.78190 162.3105 ## 3 Male 74.11011 212.7409 ## 4 Male 71.73098 220.0425 ## 5 Male 69.88180 206.3498 ## 6 Male 67.25302 152.2122 # a) model1 &lt;- lm(Height ~ Gender, data = htwtgen) model2 &lt;- lm(Height ~ Gender + Weight, data = htwtgen) model3 &lt;- lm(Height ~ Gender * Weight, data = htwtgen) # print(summary(model1)) # print(summary(model2)) # print(summary(model3)) use equations to find the height for men and women respectively: \\[E(height|male\\; and \\; weight=x)=47.34778 - 1.68367 + 0.12043x + 0.00449x = 45.7 + 0.125x\\] \\[E(height|female\\; and \\; weight=x)=47.34778 + 0.12043x\\] # for men new.obs &lt;- data.frame(Weight=120, Gender=&quot;Male&quot;) predict(model3, newdata = new.obs) ## 1 ## 60.65427 # for female new.obs &lt;- data.frame(Weight=120, Gender=&quot;Female&quot;) predict(model3, newdata = new.obs) ## 1 ## 61.79882 Exr. 9.2 # read in data and show preview trout &lt;- read.csv(&quot;data/lm/trout.csv&quot;) # recode the Group variable and treat like categories (factor) trout$Group &lt;- factor(trout$Group, labels=c(&quot;Dominant&quot;, &quot;Subordinate&quot;)) head(trout) ## Energy Ration Group ## 1 44.26 81.35 Dominant ## 2 67.16 91.68 Dominant ## 3 48.15 58.00 Dominant ## 4 34.53 58.63 Dominant ## 5 67.93 91.93 Dominant ## 6 72.45 96.56 Dominant # plot data # boxplots of Energy and Ration per group boxplot(trout$Energy ~ trout$Group, xlab=&quot;&quot;, ylab=&quot;Energy&quot;) boxplot(trout$Ration ~ trout$Group, xlab=&quot;&quot;, ylab=&quot;Ration&quot;) # scatter plot of Ration vs. Energy plot(trout$Ration, trout$Energy, pch=19, xlab=&quot;Ration&quot;, ylab=&quot;Energy&quot;) From the exploratory plots we see that there is some sort of relationship between ratio and energy, i.e. energy increase while ration obtained increases From boxplots we see that the ration obtained may be different in two groups # Is there a relationship between ration obtained and energy expenditure model1 &lt;- lm(Energy ~ Ration, data = trout) print(summary(model1)) ## ## Call: ## lm(formula = Energy ~ Ration, data = trout) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.704 -4.703 -0.578 2.432 33.506 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.3037 12.5156 0.344 0.734930 ## Ration 0.7211 0.1716 4.203 0.000535 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.05 on 18 degrees of freedom ## Multiple R-squared: 0.4953,\tAdjusted R-squared: 0.4673 ## F-statistic: 17.66 on 1 and 18 DF, p-value: 0.0005348 # from the regression output we can see that yes, a unit increase in ratio increase energy expenditure by 0.72 # Is there a relationship between ration obtained and energy expenditure different for each type of fish? # we first check if there is a group effect model2 &lt;- lm(Energy ~ Ration + Group, data = trout) print(summary(model2)) ## ## Call: ## lm(formula = Energy ~ Ration + Group, data = trout) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.130 -5.139 -0.870 2.199 25.622 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -24.8506 13.3031 -1.868 0.07910 . ## Ration 1.0109 0.1626 6.218 9.36e-06 *** ## GroupSubordinate 17.0120 5.1075 3.331 0.00396 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.647 on 17 degrees of freedom ## Multiple R-squared: 0.6946,\tAdjusted R-squared: 0.6587 ## F-statistic: 19.33 on 2 and 17 DF, p-value: 4.182e-05 ggPredict(model2) # and whether there is interaction effect model3 &lt;- lm(Energy ~ Ration * Group, data = trout) print(summary(model3)) ## ## Call: ## lm(formula = Energy ~ Ration * Group, data = trout) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.7951 -6.0981 -0.1554 3.9612 23.5946 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.2330 15.9394 -0.579 0.570483 ## Ration 0.8149 0.1968 4.141 0.000767 *** ## GroupSubordinate -18.9558 22.6934 -0.835 0.415848 ## Ration:GroupSubordinate 0.5200 0.3204 1.623 0.124148 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.214 on 16 degrees of freedom ## Multiple R-squared: 0.7378,\tAdjusted R-squared: 0.6886 ## F-statistic: 15 on 3 and 16 DF, p-value: 6.537e-05 ggPredict(model3) Based on the regression output and plots we can say: there is relationship between ration obtained and energy expenditure that this relationship is the same in the two groups although the energy expenditure is higher in the dominant fish "],
["model-summary-assumptions.html", "Chapter 10 Model summary &amp; assumptions 10.1 Assessing model fit 10.2 \\(R^2\\): summary of the fitted model 10.3 \\(R^2\\) and correlation coefficient 10.4 \\(R^2(adj)\\) 10.5 The assumptions of a linear model 10.6 Checking assumptions 10.7 Influential observations 10.8 Exercises: linear models III Answers to selected exercises (linear models III)", " Chapter 10 Model summary &amp; assumptions Aims to introduce concepts of linear models summary and assumptions Learning outcomes to able to interpret \\(R^2\\) and \\(R^2(adj)\\) values state the assumptions of a linear model and assess them using residual plots 10.1 Assessing model fit earlier we learned how to estimate parameters in a liner model using least squares now we will consider how to assess the goodness of fit of a model we do that by calculating the amount of variability in the response that is explained by the model 10.2 \\(R^2\\): summary of the fitted model considering a simple linear regression, the simplest model, Model 0, we could consider fitting is \\[Y_i = \\beta_0+ \\epsilon_i\\] that corresponds to a line that run through the data but lies parallel to the horizontal axis in our plasma volume example that would correspond the mean value of plasma volume being predicted for any value of weight (in purple) TSS, denoted Total corrected sum-of-squares is the residual sum-of-squares for Model 0 \\[S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}\\] corresponding the to the sum of squared distances to the purple line Fitting Model 1 of the form \\[Y_i = \\beta_0 + \\beta_1x + \\epsilon_i\\] we have earlier defined RSS, the residual sum-of-squares as: \\[RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\] that corresponds to the squared distances between the observed values \\(y_i, \\dots,y_n\\) to fitted values \\(\\hat{y_1}, \\dots \\hat{y_n}\\), i.e. distances to the red fitted line Definition 10.1 A simple but useful measure of model fit is given by \\[R^2 = 1 - \\frac{RSS}{TSS}\\] where: RSS is the residual sum-of-squares for Model 1, the fitted model of interest TSS is the sum of squares of the null model \\(R^2\\) quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model \\(R^2\\) is also referred as coefficient of determination It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data Values of \\(R^2\\) approaching 1 indicate he model to be a good fit Values of \\(R^2\\) less than 0.5 suggest that the model gives rather a poor fit to the data 10.3 \\(R^2\\) and correlation coefficient Theorem 10.1 In the case of simple linear regression: Model 1: \\(Y_i = \\beta_0 + \\beta_1x + \\epsilon_i\\) \\[R^2 = r^2\\] where: \\(R^2\\) is the coefficient of determination \\(r^2\\) is the sample correlation coefficient 10.4 \\(R^2(adj)\\) in the case of multiple linear regression, where there is more than one explanatory variable in the model we are using the adjusted version of R^2 to assess the model fit as the number of explanatory variables increase, \\(R^2\\) also increases \\(R^2(adj)\\) takes this into account, i.e. adjusts for the fact that there is more than one explanatory variable in the model Theorem 10.2 For any multiple linear regression \\[Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} + \\epsilon_i\\] \\(R^2(adj)\\) is defined as \\[R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}\\] where \\(p\\) is the number of independent predictors, i.e. the number of variables in the model, excluding the constant \\(R^2(adj)\\) can also be calculated from \\(R^2\\): \\[R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}\\] We can calculate the values in R and compare the results to the output of linear regression htwtgen &lt;- read.csv(&quot;data/lm/heights_weights_genders.csv&quot;) head(htwtgen) ## Gender Height Weight ## 1 Male 73.84702 241.8936 ## 2 Male 68.78190 162.3105 ## 3 Male 74.11011 212.7409 ## 4 Male 71.73098 220.0425 ## 5 Male 69.88180 206.3498 ## 6 Male 67.25302 152.2122 attach(htwtgen) ## Simple linear regression model.simple &lt;- lm(Height ~ Weight, data=htwtgen) # TSS TSS &lt;- sum((Height - mean(Height))^2) # RSS # residuals are returned in the model type names(model.simple) RSS &lt;- sum((model.simple$residuals)^2) R2 &lt;- 1 - (RSS/TSS) print(R2) ## [1] 0.8551742 print(summary(model.simple)) ## ## Call: ## lm(formula = Height ~ Weight, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8142 -0.9907 0.0263 0.9918 5.5950 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.848e+01 7.507e-02 645.8 &lt;2e-16 *** ## Weight 1.108e-01 4.561e-04 243.0 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.464 on 9998 degrees of freedom ## Multiple R-squared: 0.8552,\tAdjusted R-squared: 0.8552 ## F-statistic: 5.904e+04 on 1 and 9998 DF, p-value: &lt; 2.2e-16 ## Multiple regression model.multiple &lt;- lm(Height ~ Weight + Gender, data=htwtgen) n &lt;- length(Weight) p &lt;- 1 RSS &lt;- sum((model.multiple$residuals)^2) R2_adj &lt;- 1 - (RSS/(n-p-1))/(TSS/(n-1)) print(R2_adj) ## [1] 0.8608793 print(summary(model.multiple)) ## ## Call: ## lm(formula = Height ~ Weight + Gender, data = htwtgen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.4956 -0.9583 0.0126 0.9867 5.8358 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.0306678 0.1025161 458.76 &lt;2e-16 *** ## Weight 0.1227594 0.0007396 165.97 &lt;2e-16 *** ## GenderMale -0.9628643 0.0474947 -20.27 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.435 on 9997 degrees of freedom ## Multiple R-squared: 0.8609,\tAdjusted R-squared: 0.8609 ## F-statistic: 3.093e+04 on 2 and 9997 DF, p-value: &lt; 2.2e-16 10.5 The assumptions of a linear model up until now we were fitting models and discussed how to assess the model fit before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data informally we have been using box plots and scatter plots to look at the data there are however formal definitions of the assumptions Assumption A: The deterministic part of the model captures all the non-random structure in the data this implies that the mean of the errors \\(\\epsilon_i\\) is zero it applies only over the range of explanatory variables Assumption B: the scale of variability of the errors is constant at all values of the explanatory variables practically we are looking at whether the observations are equally spread on both side of the regression line Assumption C: the errors are independent broadly speaking this means that knowledge of errors attached to one observation does not give us any information about the error attached to another Assumptions D: the errors are normally distributed this will allow us to describe the variation in the model’s parameters estimates and therefore make inferences about the population from which our sample was taken Assumption E: the values of the explanatory variables are recorded without error this one is not possible to check via examining the data, instead we have to consider the nature of the experiment 10.6 Checking assumptions Residuals, \\(\\hat{\\epsilon_i} = y_i - \\hat{y_i}\\) are the main ingredient to check model assumptions. We use plots such as: Histograms or normal probability plots of \\(\\hat{\\epsilon_i}\\) useful to check the assumption of normality Plots of \\(\\hat{\\epsilon_i}\\) versus the fitted values \\(\\hat{y_i}\\) used to detect changes in error variance used to check if the mean of the errors is zero Plots of \\(\\hat{\\epsilon_i}\\) vs. an explanatory variable \\(x_{ij}\\) this helps to check that the variable \\(x_j\\) has a linear relationship with the response variable Plots of \\(\\hat{\\epsilon_i}\\) vs. an explanatory variable \\(x_{kj}\\) that is not in the model this helps to check whether the additional variable \\(x_k\\) might have a relationship with the response variable Plots of \\(\\hat{\\epsilon_i}\\) in the order of the observations were collected this is useful to check whether errors might be correlated over time Let’s look at the “good” example going back to our data of protein levels during pregnancy # read in data data.protein &lt;- read.csv(&quot;data/lm/protein.csv&quot;) protein &lt;- data.protein$Protein # our Y gestation &lt;- data.protein$Gestation # our X model &lt;- lm(protein ~ gestation) # plot diagnostic plots of the linear model # by default plot(model) calls four diagnostics plots # par() divides plot window in 2 x 2 grid par(mfrow=c(2,2)) plot(model) the residual plots provides examples of a situation where the assumptions appear to be met the linear regression appears to describe data quite well there is no obvious trend of any kind in the residuals vs. fitted values (the shape is scatted) points lie reasonably well along the line in the normal probability plot, hence normality appears to be met Examples of assumptions not being met Figure 10.1: Example of data with a typical seasonal variation (up and down) coupled wtih a linear trend. The blue line gives the linear regression fit to the data, which clearly is not adequate. In comparison, if we used a non-parametric fit, we will get the red line as the fitted relationship. The residual plot retains pattern, given by orange line, indicating that the linear model is not appropriate in this case. Figure 10.2: Example of non-constant variance Figure 10.3: Example of residulas deviating from QQ plot, i.e. not following normal distribution. The residuals can deviate in both upper and lower tail. On the left tails are lighter meaning that they have smaller values that what would be expected, on the right there are heavier tails with values larger than expected 10.7 Influential observations Sometimes individual observations can exert a great deal of influence on the fitted model One routine way of checking for this is to fit the model \\(n\\) times, missing out each observation in turn If we removed i-th observation and compared the fitted value from the full model, say \\(\\hat{y_j}\\) to those obtained by removing this point, denoted \\(\\hat{y_{j(i)}}\\) then observations with a high Cook’s distance (measuring the effect of deleting a given observation) could be influential Let’s remove some observation with higher Cook’s distance from protein data set, re-fit our model and compare the diagnostics plots # observations to be removed (based on Residuals vs. Leverage plot) obs &lt;- c(18,7) # fit models removing observations model.2 &lt;- lm(protein[-obs] ~ gestation[-obs]) # plot diagnostics plot par(mfrow=c(2,2)) plot(model.2) 10.8 Exercises: linear models III Exercise 10.1 Brozek score Researchers collected age, weight, height and 10 body circumference measurements for 252 men in an attempt to find an alternative way of calculate body fat as oppose to measuring someone weight and volume, the latter one by submerging in a water tank. Is it possible to predict body fat using easy-to-record measurements? Use lm() function and fit a linear method to model brozek, score estimate of percent body fat find \\(R^2\\) and \\(R^2(adj)\\) assess the diagnostics plots to check for model assumptions delete observation #86 with the highest Cook’s distance and re-fit the model (model.clean) look at the model summary. Are all variables associated with brozek score? try improving the model fit by removing variables with the highest p-value first and re-fitting the model until all the variables are significantly associated with the response (p value less than 0.1); note down the \\(R^2(adj)\\) values while doing so compare the output models for model.clean and final model To access and preview the data: data(fat, package = &quot;faraway&quot;) Answers to selected exercises (linear models III) Exr. 10.1 # access and preview data data(fat, package = &quot;faraway&quot;) head(fat) ## brozek siri density age weight height adipos free neck chest abdom hip ## 1 12.6 12.3 1.0708 23 154.25 67.75 23.7 134.9 36.2 93.1 85.2 94.5 ## 2 6.9 6.1 1.0853 22 173.25 72.25 23.4 161.3 38.5 93.6 83.0 98.7 ## 3 24.6 25.3 1.0414 22 154.00 66.25 24.7 116.0 34.0 95.8 87.9 99.2 ## 4 10.9 10.4 1.0751 26 184.75 72.25 24.9 164.7 37.4 101.8 86.4 101.2 ## 5 27.8 28.7 1.0340 24 184.25 71.25 25.6 133.1 34.4 97.3 100.0 101.9 ## 6 20.6 20.9 1.0502 24 210.25 74.75 26.5 167.0 39.0 104.5 94.4 107.8 ## thigh knee ankle biceps forearm wrist ## 1 59.0 37.3 21.9 32.0 27.4 17.1 ## 2 58.7 37.3 23.4 30.5 28.9 18.2 ## 3 59.6 38.9 24.0 28.8 25.2 16.6 ## 4 60.1 37.3 22.8 32.4 29.4 18.2 ## 5 63.2 42.2 24.0 32.2 27.7 17.7 ## 6 66.0 42.0 25.6 35.7 30.6 18.8 # fit linear regression model model.all &lt;- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) # print model summary print(summary(model.all)) ## ## Call: ## lm(formula = brozek ~ age + weight + height + neck + abdom + ## hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2664 -2.5658 -0.0798 2.8976 9.3204 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.063433 14.489336 -1.178 0.24011 ## age 0.056520 0.029888 1.891 0.05983 . ## weight -0.085513 0.045170 -1.893 0.05954 . ## height -0.059703 0.086695 -0.689 0.49171 ## neck -0.439315 0.214802 -2.045 0.04193 * ## abdom 0.875779 0.070589 12.407 &lt; 2e-16 *** ## hip -0.192118 0.132655 -1.448 0.14885 ## thigh 0.237304 0.131793 1.801 0.07303 . ## knee -0.006595 0.222832 -0.030 0.97642 ## ankle 0.164831 0.204681 0.805 0.42144 ## biceps 0.149530 0.157693 0.948 0.34397 ## forearm 0.424885 0.182801 2.324 0.02095 * ## wrist -1.474317 0.494475 -2.982 0.00316 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.98 on 239 degrees of freedom ## Multiple R-squared: 0.7489,\tAdjusted R-squared: 0.7363 ## F-statistic: 59.4 on 12 and 239 DF, p-value: &lt; 2.2e-16 # diagnostics plots par(mfrow=c(2,2)) plot(model.all) # remove potentially influential observations obs &lt;- c(86) fat2 &lt;- fat[-obs, ] # re-fit the model model.clean &lt;- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) # diagnostics plots par(mfrow=c(2,2)) plot(model.clean) # model summary print(summary(model.clean)) ## ## Call: ## lm(formula = brozek ~ age + weight + height + neck + abdom + ## hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2664 -2.5658 -0.0798 2.8976 9.3204 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.063433 14.489336 -1.178 0.24011 ## age 0.056520 0.029888 1.891 0.05983 . ## weight -0.085513 0.045170 -1.893 0.05954 . ## height -0.059703 0.086695 -0.689 0.49171 ## neck -0.439315 0.214802 -2.045 0.04193 * ## abdom 0.875779 0.070589 12.407 &lt; 2e-16 *** ## hip -0.192118 0.132655 -1.448 0.14885 ## thigh 0.237304 0.131793 1.801 0.07303 . ## knee -0.006595 0.222832 -0.030 0.97642 ## ankle 0.164831 0.204681 0.805 0.42144 ## biceps 0.149530 0.157693 0.948 0.34397 ## forearm 0.424885 0.182801 2.324 0.02095 * ## wrist -1.474317 0.494475 -2.982 0.00316 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.98 on 239 degrees of freedom ## Multiple R-squared: 0.7489,\tAdjusted R-squared: 0.7363 ## F-statistic: 59.4 on 12 and 239 DF, p-value: &lt; 2.2e-16 # re-fit the model (no height) model.red1 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) print(summary(model.red1)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + ## knee + ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2830 -2.6162 -0.1017 2.8789 9.3713 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -22.66569 11.97691 -1.892 0.05963 . ## age 0.05948 0.02954 2.013 0.04521 * ## weight -0.09829 0.04114 -2.389 0.01765 * ## neck -0.43444 0.21445 -2.026 0.04389 * ## abdom 0.88762 0.06839 12.979 &lt; 2e-16 *** ## hip -0.17180 0.12919 -1.330 0.18483 ## thigh 0.25327 0.12960 1.954 0.05183 . ## knee -0.02318 0.22128 -0.105 0.91665 ## ankle 0.17300 0.20411 0.848 0.39752 ## biceps 0.15695 0.15715 0.999 0.31894 ## forearm 0.43091 0.18239 2.363 0.01895 * ## wrist -1.51011 0.49120 -3.074 0.00235 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.976 on 240 degrees of freedom ## Multiple R-squared: 0.7484,\tAdjusted R-squared: 0.7369 ## F-statistic: 64.9 on 11 and 240 DF, p-value: &lt; 2.2e-16 # re-fit the model (no knee) model.red2 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + ankle + biceps + forearm + wrist, data = fat) print(summary(model.red2)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + ## ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2552 -2.5979 -0.1133 2.8693 9.3584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -23.08716 11.25781 -2.051 0.04137 * ## age 0.05875 0.02864 2.051 0.04134 * ## weight -0.09965 0.03897 -2.557 0.01117 * ## neck -0.43088 0.21131 -2.039 0.04253 * ## abdom 0.88875 0.06740 13.186 &lt; 2e-16 *** ## hip -0.17231 0.12884 -1.337 0.18234 ## thigh 0.24942 0.12403 2.011 0.04544 * ## ankle 0.16946 0.20089 0.844 0.39974 ## biceps 0.15847 0.15616 1.015 0.31123 ## forearm 0.42946 0.18150 2.366 0.01876 * ## wrist -1.51470 0.48823 -3.102 0.00215 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.968 on 241 degrees of freedom ## Multiple R-squared: 0.7484,\tAdjusted R-squared: 0.738 ## F-statistic: 71.69 on 10 and 241 DF, p-value: &lt; 2.2e-16 # re-fit the model (no ankle) model.red3 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + biceps + forearm + wrist, data = fat) print(summary(model.red3)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + ## biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0740 -2.5615 -0.1021 2.7999 9.3199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -20.61247 10.86240 -1.898 0.0589 . ## age 0.05727 0.02857 2.004 0.0461 * ## weight -0.09141 0.03770 -2.424 0.0161 * ## neck -0.45458 0.20931 -2.172 0.0308 * ## abdom 0.88098 0.06673 13.203 &lt;2e-16 *** ## hip -0.17575 0.12870 -1.366 0.1733 ## thigh 0.25504 0.12378 2.061 0.0404 * ## biceps 0.15178 0.15587 0.974 0.3311 ## forearm 0.42805 0.18138 2.360 0.0191 * ## wrist -1.40948 0.47175 -2.988 0.0031 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.965 on 242 degrees of freedom ## Multiple R-squared: 0.7477,\tAdjusted R-squared: 0.7383 ## F-statistic: 79.67 on 9 and 242 DF, p-value: &lt; 2.2e-16 # re-fit the model (no biceps) model.red4 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + forearm + wrist, data = fat) print(summary(model.red4)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + ## forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0574 -2.7411 -0.1912 2.6929 9.4977 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -20.06213 10.84654 -1.850 0.06558 . ## age 0.05922 0.02850 2.078 0.03876 * ## weight -0.08414 0.03695 -2.277 0.02366 * ## neck -0.43189 0.20799 -2.077 0.03889 * ## abdom 0.87721 0.06661 13.170 &lt; 2e-16 *** ## hip -0.18641 0.12821 -1.454 0.14727 ## thigh 0.28644 0.11949 2.397 0.01727 * ## forearm 0.48255 0.17251 2.797 0.00557 ** ## wrist -1.40487 0.47167 -2.978 0.00319 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.965 on 243 degrees of freedom ## Multiple R-squared: 0.7467,\tAdjusted R-squared: 0.7383 ## F-statistic: 89.53 on 8 and 243 DF, p-value: &lt; 2.2e-16 # re-fit the model (no hip) model.red5 &lt;- lm(brozek ~ age + weight + neck + abdom + thigh + forearm + wrist, data = fat) print(summary(model.red5)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + ## wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0193 -2.8016 -0.1234 2.9387 9.0019 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -30.17420 8.34200 -3.617 0.000362 *** ## age 0.06149 0.02852 2.156 0.032047 * ## weight -0.11236 0.03151 -3.565 0.000437 *** ## neck -0.37203 0.20434 -1.821 0.069876 . ## abdom 0.85152 0.06437 13.229 &lt; 2e-16 *** ## thigh 0.20973 0.10745 1.952 0.052099 . ## forearm 0.51824 0.17115 3.028 0.002726 ** ## wrist -1.40081 0.47274 -2.963 0.003346 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.974 on 244 degrees of freedom ## Multiple R-squared: 0.7445,\tAdjusted R-squared: 0.7371 ## F-statistic: 101.6 on 7 and 244 DF, p-value: &lt; 2.2e-16 # compare model.clean and final model print(summary(model.clean)) ## ## Call: ## lm(formula = brozek ~ age + weight + height + neck + abdom + ## hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2664 -2.5658 -0.0798 2.8976 9.3204 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.063433 14.489336 -1.178 0.24011 ## age 0.056520 0.029888 1.891 0.05983 . ## weight -0.085513 0.045170 -1.893 0.05954 . ## height -0.059703 0.086695 -0.689 0.49171 ## neck -0.439315 0.214802 -2.045 0.04193 * ## abdom 0.875779 0.070589 12.407 &lt; 2e-16 *** ## hip -0.192118 0.132655 -1.448 0.14885 ## thigh 0.237304 0.131793 1.801 0.07303 . ## knee -0.006595 0.222832 -0.030 0.97642 ## ankle 0.164831 0.204681 0.805 0.42144 ## biceps 0.149530 0.157693 0.948 0.34397 ## forearm 0.424885 0.182801 2.324 0.02095 * ## wrist -1.474317 0.494475 -2.982 0.00316 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.98 on 239 degrees of freedom ## Multiple R-squared: 0.7489,\tAdjusted R-squared: 0.7363 ## F-statistic: 59.4 on 12 and 239 DF, p-value: &lt; 2.2e-16 print(summary(model.red5)) ## ## Call: ## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + ## wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0193 -2.8016 -0.1234 2.9387 9.0019 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -30.17420 8.34200 -3.617 0.000362 *** ## age 0.06149 0.02852 2.156 0.032047 * ## weight -0.11236 0.03151 -3.565 0.000437 *** ## neck -0.37203 0.20434 -1.821 0.069876 . ## abdom 0.85152 0.06437 13.229 &lt; 2e-16 *** ## thigh 0.20973 0.10745 1.952 0.052099 . ## forearm 0.51824 0.17115 3.028 0.002726 ** ## wrist -1.40081 0.47274 -2.963 0.003346 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.974 on 244 degrees of freedom ## Multiple R-squared: 0.7445,\tAdjusted R-squared: 0.7371 ## F-statistic: 101.6 on 7 and 244 DF, p-value: &lt; 2.2e-16 Note: we have just run a very simple feature selection using stepwise regression. In this method, using backward elimination, we build a model containing all the variables and remove them one by one based on defined criteria (here we have used p-values) and we stop when we have a justifiable model or when removing a predictor does not change the chosen criterion significantly. "],
["generalized-linear-models.html", "Chapter 11 Generalized linear models", " Chapter 11 Generalized linear models Aims xxx Learning outcomes xxx "],
["linear-mixed-models.html", "Chapter 12 Linear Mixed Models", " Chapter 12 Linear Mixed Models Aims xxx Learning outcomes xxx "]
]
